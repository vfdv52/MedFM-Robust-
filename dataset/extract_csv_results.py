#!/usr/bin/env python3
"""
CSVç»“æœæå–å·¥å…·
================
ä» results æ–‡ä»¶å¤¹ä¸­æå–æ‰€æœ‰CSVæ–‡ä»¶ï¼Œå¹¶ä¿ç•™å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
- æ•°æ®é›†åç§° (dataset)
- æ¨¡å‹åç§° (model)
- å¾®è°ƒç­–ç•¥ (strategy)
- è¯„ä¼°ç±»å‹ (adversarial/perturbation)

è¾“å‡ºç»“æ„:
extracted_csv/
â”œâ”€â”€ index.csv                   # ç´¢å¼•æ–‡ä»¶ï¼Œè®°å½•æ‰€æœ‰CSVçš„å…ƒä¿¡æ¯
â”œâ”€â”€ medsam/
â”‚   â”œâ”€â”€ isic_2016/
â”‚   â”‚   â”œâ”€â”€ decoder_only/
â”‚   â”‚   â”‚   â”œâ”€â”€ results_adversarial_xxx.csv
â”‚   â”‚   â”‚   â””â”€â”€ results_adversarial_xxx_SUMMARY.csv
â”‚   â”‚   â””â”€â”€ lora/
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ brain-tumor/
â”‚       â””â”€â”€ ...
â””â”€â”€ sammed2d/
    â””â”€â”€ ...

ä½¿ç”¨æ–¹æ³•:
    python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv
    python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv --flat  # æ‰å¹³åŒ–è¾“å‡º
"""

# # åŸºæœ¬ç”¨æ³• - ä¿æŒå±‚çº§ç»“æ„
# python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv

# # åªæå–æ±‡æ€»æ–‡ä»¶ (æœ€å®ç”¨ - æ–‡ä»¶å°)
# python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv --summary_only

# # æ‰å¹³åŒ–è¾“å‡º (æ‰€æœ‰æ–‡ä»¶æ”¾ä¸€ä¸ªç›®å½•ï¼Œæ–‡ä»¶ååŒ…å«æ‰€æœ‰ä¿¡æ¯)
# python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv --flat --summary_only

# # åŒæ—¶ç”Ÿæˆåˆå¹¶å¯¹æ¯”è¡¨
# python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv --summary_only --merge_summary
# ```

# **è¾“å‡ºç»“æ„:**
# ```
# extracted_csv/
# â”œâ”€â”€ index.csv              # ç´¢å¼•æ–‡ä»¶ï¼Œè®°å½•æ‰€æœ‰CSVçš„å…ƒä¿¡æ¯
# â”œâ”€â”€ index.json             # JSONæ ¼å¼ç´¢å¼•
# â”œâ”€â”€ merged_summary_adversarial.csv   # (--merge_summaryæ—¶ç”Ÿæˆ)
# â”œâ”€â”€ merged_summary_perturbation.csv  # (--merge_summaryæ—¶ç”Ÿæˆ)
# â””â”€â”€ medsam/
#     â””â”€â”€ isic_2016/
#         â””â”€â”€ decoder_only/
#             â””â”€â”€ results_adversarial_xxx_SUMMARY.csv

import os
import re
import shutil
import argparse
import json
from pathlib import Path
from datetime import datetime
import csv


def parse_experiment_path(csv_path: str) -> dict:
    """
    ä»CSVè·¯å¾„è§£æå®éªŒå…ƒä¿¡æ¯
    
    è·¯å¾„æ¨¡å¼ç¤ºä¾‹:
    ./results/20250120_143000_decoder_only/pipeline_medsam_isic_2016/results/results_adversarial_xxx.csv
    ./results/20250120_143000_pretrained/pipeline_sammed2d_brain-tumor/results/results_perturbation_xxx.csv
    """
    path_parts = Path(csv_path).parts
    info = {
        'csv_path': csv_path,
        'csv_name': os.path.basename(csv_path),
        'strategy': 'unknown',
        'model': 'unknown',
        'dataset': 'unknown',
        'eval_type': 'unknown',
        'timestamp': 'unknown'
    }
    
    # 1. ä»ä¸»ç›®å½•åæå– timestamp å’Œ strategy
    # æ ¼å¼: {timestamp}_{strategy} æˆ– {timestamp}_pretrained
    for part in path_parts:
        # åŒ¹é… 20250120_143000_decoder_only è¿™æ ·çš„æ ¼å¼
        match = re.match(r'^(\d{8}_\d{6})_(.+)$', part)
        if match:
            info['timestamp'] = match.group(1)
            info['strategy'] = match.group(2)
            break
    
    # 2. ä» pipeline_* ç›®å½•åæå– model å’Œ dataset
    # æ ¼å¼: pipeline_{model}_{dataset}
    for part in path_parts:
        match = re.match(r'^pipeline_([^_]+)_(.+)$', part)
        if match:
            info['model'] = match.group(1)
            info['dataset'] = match.group(2)
            break
    
    # 3. ä»CSVæ–‡ä»¶åæå– eval_type
    # æ ¼å¼: results_adversarial_xxx.csv æˆ– results_perturbation_xxx.csv
    csv_name = info['csv_name']
    if 'adversarial' in csv_name.lower():
        info['eval_type'] = 'adversarial'
    elif 'perturbation' in csv_name.lower():
        info['eval_type'] = 'perturbation'
    
    # 4. åˆ¤æ–­æ˜¯å¦ä¸ºæ±‡æ€»æ–‡ä»¶
    info['is_summary'] = 'SUMMARY' in csv_name or 'STATS' in csv_name
    
    return info


def find_all_csv_files(results_dir: str) -> list:
    """é€’å½’æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶"""
    csv_files = []
    for root, dirs, files in os.walk(results_dir):
        for file in files:
            if file.endswith('.csv'):
                full_path = os.path.join(root, file)
                csv_files.append(full_path)
    return csv_files


def generate_output_filename(info: dict, flat: bool = False) -> str:
    """
    ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    
    å±‚çº§æ¨¡å¼: {model}/{dataset}/{strategy}/{åŸå§‹æ–‡ä»¶å}
    æ‰å¹³æ¨¡å¼: {model}__{dataset}__{strategy}__{eval_type}__{åŸå§‹æ–‡ä»¶å}
    """
    if flat:
        # æ‰å¹³åŒ–å‘½åï¼Œä¾¿äºå¿«é€ŸæŸ¥çœ‹
        parts = [
            info['model'],
            info['dataset'],
            info['strategy'],
            info['eval_type']
        ]
        base_name = info['csv_name']
        return f"{'__'.join(parts)}__{base_name}"
    else:
        # å±‚çº§ç»“æ„
        return os.path.join(
            info['model'],
            info['dataset'],
            info['strategy'],
            info['csv_name']
        )


def extract_csv_files(results_dir: str, output_dir: str, flat: bool = False, 
                       summary_only: bool = False, detail_only: bool = False):
    """
    ä¸»æå–å‡½æ•°
    
    Args:
        results_dir: ç»“æœç›®å½• (./results)
        output_dir: è¾“å‡ºç›®å½•
        flat: æ˜¯å¦ä½¿ç”¨æ‰å¹³åŒ–å‘½å
        summary_only: åªæå–æ±‡æ€»æ–‡ä»¶ (*_SUMMARY.csv, *_STATS*.csv)
        detail_only: åªæå–è¯¦ç»†ç»“æœæ–‡ä»¶ (éæ±‡æ€»æ–‡ä»¶)
    """
    print(f"ğŸ“ æ‰«æç›®å½•: {results_dir}")
    csv_files = find_all_csv_files(results_dir)
    print(f"âœ… æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    if not csv_files:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶")
        return
    
    # è§£ææ‰€æœ‰CSVæ–‡ä»¶çš„å…ƒä¿¡æ¯
    all_info = []
    for csv_path in csv_files:
        info = parse_experiment_path(csv_path)
        all_info.append(info)
    
    # è¿‡æ»¤
    if summary_only:
        all_info = [info for info in all_info if info['is_summary']]
        print(f"ğŸ“Š ç­›é€‰æ±‡æ€»æ–‡ä»¶: {len(all_info)} ä¸ª")
    elif detail_only:
        all_info = [info for info in all_info if not info['is_summary']]
        print(f"ğŸ“‹ ç­›é€‰è¯¦ç»†æ–‡ä»¶: {len(all_info)} ä¸ª")
    
    if not all_info:
        print("âš ï¸ ç­›é€‰åæ— æ–‡ä»¶å¯æå–")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # å¤åˆ¶æ–‡ä»¶
    copied_count = 0
    index_data = []
    
    for info in all_info:
        src_path = info['csv_path']
        rel_output = generate_output_filename(info, flat)
        dst_path = os.path.join(output_dir, rel_output)
        
        # åˆ›å»ºç›®æ ‡ç›®å½•
        os.makedirs(os.path.dirname(dst_path), exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶
        try:
            shutil.copy2(src_path, dst_path)
            copied_count += 1
            
            # è®°å½•ç´¢å¼•
            index_data.append({
                'model': info['model'],
                'dataset': info['dataset'],
                'strategy': info['strategy'],
                'eval_type': info['eval_type'],
                'is_summary': info['is_summary'],
                'timestamp': info['timestamp'],
                'output_path': rel_output,
                'original_path': src_path
            })
            
            print(f"âœ“ [{info['model']}][{info['dataset']}][{info['strategy']}] {info['csv_name']}")
            
        except Exception as e:
            print(f"âœ— å¤åˆ¶å¤±è´¥: {src_path} -> {e}")
    
    # ä¿å­˜ç´¢å¼•æ–‡ä»¶
    index_path = os.path.join(output_dir, 'index.csv')
    with open(index_path, 'w', newline='', encoding='utf-8') as f:
        if index_data:
            writer = csv.DictWriter(f, fieldnames=index_data[0].keys())
            writer.writeheader()
            writer.writerows(index_data)
    
    # åŒæ—¶ä¿å­˜JSONæ ¼å¼çš„ç´¢å¼•
    index_json_path = os.path.join(output_dir, 'index.json')
    with open(index_json_path, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æå–å®Œæˆ!")
    print(f"{'='*60}")
    print(f"   æ€»è®¡å¤åˆ¶: {copied_count} ä¸ªæ–‡ä»¶")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   ç´¢å¼•æ–‡ä»¶: {index_path}")
    
    # æŒ‰ç»´åº¦ç»Ÿè®¡
    models = set(info['model'] for info in index_data)
    datasets = set(info['dataset'] for info in index_data)
    strategies = set(info['strategy'] for info in index_data)
    
    print(f"\nğŸ“ˆ ç»´åº¦ç»Ÿè®¡:")
    print(f"   æ¨¡å‹: {len(models)} ä¸ª - {', '.join(sorted(models))}")
    print(f"   æ•°æ®é›†: {len(datasets)} ä¸ª - {', '.join(sorted(datasets))}")
    print(f"   ç­–ç•¥: {len(strategies)} ä¸ª - {', '.join(sorted(strategies))}")
    print(f"{'='*60}\n")
    
    return index_data


def create_merged_summary(output_dir: str, index_data: list):
    """
    åˆ›å»ºåˆå¹¶çš„æ±‡æ€»è¡¨ï¼Œæ–¹ä¾¿å¿«é€Ÿå¯¹æ¯”ä¸åŒç­–ç•¥çš„æ€§èƒ½
    
    è¾“å‡º:
    - merged_summary_adversarial.csv: æ‰€æœ‰å¯¹æŠ—æ”»å‡»çš„æ±‡æ€»æ•°æ®
    - merged_summary_perturbation.csv: æ‰€æœ‰æ‰°åŠ¨è¯„ä¼°çš„æ±‡æ€»æ•°æ®
    """
    import pandas as pd
    
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆåˆå¹¶æ±‡æ€»è¡¨...")
    
    # åªå¤„ç†æ±‡æ€»æ–‡ä»¶
    summary_files = [info for info in index_data if info['is_summary'] and 'SUMMARY' in info['output_path']]
    
    if not summary_files:
        print("âš ï¸ æœªæ‰¾åˆ°æ±‡æ€»æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶")
        return
    
    # åˆ†åˆ«å¤„ç† adversarial å’Œ perturbation
    for eval_type in ['adversarial', 'perturbation']:
        type_files = [info for info in summary_files if info['eval_type'] == eval_type]
        
        if not type_files:
            continue
        
        merged_rows = []
        for info in type_files:
            csv_path = os.path.join(output_dir, info['output_path'])
            try:
                df = pd.read_csv(csv_path)
                # æ·»åŠ å…ƒä¿¡æ¯åˆ—
                df['Model'] = info['model']
                df['Dataset'] = info['dataset']
                df['Strategy'] = info['strategy']
                df['Timestamp'] = info['timestamp']
                merged_rows.append(df)
            except Exception as e:
                print(f"âš ï¸ è¯»å–å¤±è´¥: {csv_path} - {e}")
        
        if merged_rows:
            merged_df = pd.concat(merged_rows, ignore_index=True)
            # é‡æ’åˆ—é¡ºåºï¼Œå…ƒä¿¡æ¯æ”¾å‰é¢
            cols = ['Model', 'Dataset', 'Strategy', 'Timestamp'] + \
                   [c for c in merged_df.columns if c not in ['Model', 'Dataset', 'Strategy', 'Timestamp']]
            merged_df = merged_df[cols]
            
            output_path = os.path.join(output_dir, f'merged_summary_{eval_type}.csv')
            merged_df.to_csv(output_path, index=False, float_format='%.4f')
            print(f"âœ… åˆå¹¶æ±‡æ€»è¡¨: {output_path} ({len(merged_df)} è¡Œ)")


def main():
    parser = argparse.ArgumentParser(
        description="ä»resultsç›®å½•æå–CSVæ–‡ä»¶ï¼Œä¿ç•™æ•°æ®é›†/æ¨¡å‹/ç­–ç•¥ä¿¡æ¯",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
---------
# åŸºæœ¬ç”¨æ³• - å±‚çº§ç›®å½•ç»“æ„
python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv

# æ‰å¹³åŒ–è¾“å‡º - æ‰€æœ‰æ–‡ä»¶æ”¾åœ¨ä¸€ä¸ªç›®å½•ä¸‹ï¼Œæ–‡ä»¶ååŒ…å«æ‰€æœ‰ä¿¡æ¯
python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv --flat

# åªæå–æ±‡æ€»æ–‡ä»¶ (SUMMARY.csv)
python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv --summary_only

# åªæå–è¯¦ç»†ç»“æœæ–‡ä»¶ (ä¸å«SUMMARY)
python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv --detail_only

# ç”Ÿæˆåˆå¹¶æ±‡æ€»è¡¨
python extract_csv_results.py --results_dir ./results --output_dir ./extracted_csv --merge_summary
        """
    )
    
    parser.add_argument('--results_dir', type=str, default='./results',
                        help='ç»“æœç›®å½•è·¯å¾„ (é»˜è®¤: ./results)')
    parser.add_argument('--output_dir', type=str, default='./extracted_csv',
                        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: ./extracted_csv)')
    parser.add_argument('--flat', action='store_true',
                        help='ä½¿ç”¨æ‰å¹³åŒ–å‘½å (æ‰€æœ‰æ–‡ä»¶æ”¾åŒä¸€ç›®å½•)')
    parser.add_argument('--summary_only', action='store_true',
                        help='åªæå–æ±‡æ€»æ–‡ä»¶ (*_SUMMARY.csv, *_STATS*.csv)')
    parser.add_argument('--detail_only', action='store_true',
                        help='åªæå–è¯¦ç»†ç»“æœæ–‡ä»¶ (éæ±‡æ€»æ–‡ä»¶)')
    parser.add_argument('--merge_summary', action='store_true',
                        help='ç”Ÿæˆåˆå¹¶çš„æ±‡æ€»è¡¨ (éœ€è¦pandas)')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.summary_only and args.detail_only:
        print("âŒ é”™è¯¯: --summary_only å’Œ --detail_only ä¸èƒ½åŒæ—¶ä½¿ç”¨")
        return
    
    if not os.path.exists(args.results_dir):
        print(f"âŒ é”™è¯¯: ç»“æœç›®å½•ä¸å­˜åœ¨: {args.results_dir}")
        return
    
    # æ‰§è¡Œæå–
    index_data = extract_csv_files(
        results_dir=args.results_dir,
        output_dir=args.output_dir,
        flat=args.flat,
        summary_only=args.summary_only,
        detail_only=args.detail_only
    )
    
    # å¯é€‰: ç”Ÿæˆåˆå¹¶æ±‡æ€»è¡¨
    if args.merge_summary and index_data:
        try:
            create_merged_summary(args.output_dir, index_data)
        except ImportError:
            print("âš ï¸ åˆå¹¶æ±‡æ€»éœ€è¦ pandas åº“ï¼Œè¯·å®‰è£…: pip install pandas")


if __name__ == "__main__":
    main()
