#!/usr/bin/env python3
"""
å¾®è°ƒè®­ç»ƒè„šæœ¬ - æ”¯æŒMedSAMå’ŒSAM-Med2Dçš„å¾®è°ƒ
åŸºäºç°æœ‰pipelineæœ€å°åŒ–ä¿®æ”¹

ä½¿ç”¨ç¤ºä¾‹:
=========
# 1. Decoder-onlyå¾®è°ƒ (æœ€å¿«ï¼Œæ¨èå°æ•°æ®é›†)
python finetune.py \
    --model_name medsam \
    --strategy decoder_only \
    --data_path /path/to/data \
    --num_epochs 10

# 2. Adapterå¾®è°ƒ (SAM-Med2Dä¸“ç”¨)
python finetune.py \
    --model_name sammed2d \
    --strategy adapter_only \
    --data_path /path/to/data \
    --num_epochs 20

# 3. éƒ¨åˆ†Encoderå¾®è°ƒ
python finetune.py \
    --model_name medsam \
    --strategy encoder_partial \
    --unfreeze_encoder_layers 4 \
    --data_path /path/to/data

# 4. LoRAå¾®è°ƒ (å‚æ•°é«˜æ•ˆ)
python finetune.py \
    --model_name medsam \
    --strategy lora \
    --lora_r 8 \
    --data_path /path/to/data
"""

import os
import sys
import json
import time
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
from datetime import datetime
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm

# å¯¼å…¥ç°æœ‰æ¨¡å—
from model_zoo import create_model, list_available_models
from finetune_utils import (
    FinetuneConfig, 
    setup_finetune, 
    build_optimizer, 
    build_scheduler,
    SegmentationLoss,
    save_finetuned_model,
    load_finetuned_model,
    recommend_strategy,
    print_trainable_parameters
)


# ========================== æ•°æ®é›†ç±» ==========================

class SegmentationDataset(Dataset):
    """
    é€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†
    æ”¯æŒçš„æ•°æ®æ ¼å¼:
    - å›¾åƒ: jpg, png, npy
    - æ©ç : png (äºŒå€¼)
    - bbox: jsonæ–‡ä»¶
    """
    def __init__(
        self,
        img_dir: str,
        mask_dir: str,
        bbox_json: str,
        image_size: int = 256,
        is_train: bool = True,
        augmentation: bool = True,
        model_name: str = "medsam"
    ):
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.image_size = image_size
        self.is_train = is_train
        self.augmentation = augmentation and is_train
        self.model_name = model_name
        
        # åŠ è½½bbox
        self.bbox_dict = {}
        if os.path.exists(bbox_json):
            with open(bbox_json, 'r') as f:
                self.bbox_dict = json.load(f)
        
        # æ”¶é›†æœ‰æ•ˆçš„å›¾åƒ-æ©ç å¯¹
        self.samples = self._collect_samples()
        print(f"âœ… æ•°æ®é›†åŠ è½½: {len(self.samples)} ä¸ªæ ·æœ¬ ({'è®­ç»ƒ' if is_train else 'éªŒè¯'})")
        
    def _collect_samples(self) -> List[Dict]:
        """æ”¶é›†æœ‰æ•ˆçš„å›¾åƒ-æ©ç å¯¹"""
        samples = []
        extensions = ['.jpg', '.jpeg', '.png', '.npy']
        
        for img_name in os.listdir(self.img_dir):
            if not any(img_name.lower().endswith(ext) for ext in extensions):
                continue
                
            base_name = os.path.splitext(img_name)[0]
            
            # æŸ¥æ‰¾å¯¹åº”çš„æ©ç 
            mask_path = None
            # for mask_suffix in ['_Segmentation.png', '_mask.png', '.png']:
            for mask_suffix in ['_Segmentation.png', '_mask.png', '.png', '.jpg', '.jpeg']:
                potential_mask = os.path.join(self.mask_dir, base_name + mask_suffix)
                if os.path.exists(potential_mask):
                    mask_path = potential_mask
                    break
            
            if mask_path is None:
                continue
                
            # è·å–bbox
            mask_name = os.path.basename(mask_path)
            bbox = self.bbox_dict.get(mask_name, None)
            
            samples.append({
                'img_path': os.path.join(self.img_dir, img_name),
                'mask_path': mask_path,
                'bbox': bbox,
                'base_name': base_name
            })
            
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def _load_image(self, path: str) -> np.ndarray:
        """åŠ è½½å›¾åƒ"""
        if path.endswith('.npy'):
            img = np.load(path)
            if len(img.shape) == 2:
                img = np.stack([img] * 3, axis=-1)
        else:
            img = cv2.imread(path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        return img.astype(np.uint8)
    
    def _load_mask(self, path: str) -> np.ndarray:
        """åŠ è½½æ©ç """
        mask = cv2.imread(path, 0)
        mask = (mask > 0).astype(np.float32)
        return mask
    
    def _resize(self, img: np.ndarray, mask: np.ndarray, bbox: list) -> Tuple:
        """è°ƒæ•´å°ºå¯¸"""
        orig_h, orig_w = img.shape[:2]
        target_size = (self.image_size, self.image_size)
        
        img = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)
        mask = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)
        
        # ç¼©æ”¾bbox
        if bbox is not None:
            scale_w = self.image_size / orig_w
            scale_h = self.image_size / orig_h
            bbox = [
                bbox[0] * scale_w,
                bbox[1] * scale_h,
                bbox[2] * scale_w,
                bbox[3] * scale_h
            ]
            
        return img, mask, bbox
    
    def _augment(self, img: np.ndarray, mask: np.ndarray, bbox: list) -> Tuple:
        """æ•°æ®å¢å¼º"""
        if not self.augmentation:
            return img, mask, bbox
            
        h, w = img.shape[:2]
        
        # éšæœºæ°´å¹³ç¿»è½¬
        if np.random.random() < 0.5:
            img = np.fliplr(img).copy()
            mask = np.fliplr(mask).copy()
            if bbox is not None:
                bbox = [w - bbox[2], bbox[1], w - bbox[0], bbox[3]]
        
        # éšæœºå‚ç›´ç¿»è½¬
        if np.random.random() < 0.5:
            img = np.flipud(img).copy()
            mask = np.flipud(mask).copy()
            if bbox is not None:
                bbox = [bbox[0], h - bbox[3], bbox[2], h - bbox[1]]
        
        # éšæœºäº®åº¦/å¯¹æ¯”åº¦è°ƒæ•´
        if np.random.random() < 0.3:
            alpha = 0.8 + np.random.random() * 0.4  # 0.8-1.2
            beta = -20 + np.random.random() * 40    # -20 to 20
            img = np.clip(alpha * img + beta, 0, 255).astype(np.uint8)
            
        return img, mask, bbox
    
    def __getitem__(self, idx: int) -> Dict:
        sample = self.samples[idx]
        
        # åŠ è½½æ•°æ®
        img = self._load_image(sample['img_path'])
        mask = self._load_mask(sample['mask_path'])
        bbox = sample['bbox'].copy() if sample['bbox'] else None
        
        # è°ƒæ•´å°ºå¯¸
        img, mask, bbox = self._resize(img, mask, bbox)
        
        # æ•°æ®å¢å¼º
        img, mask, bbox = self._augment(img, mask, bbox)
        
        # è½¬æ¢ä¸ºtensor
        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0
        mask_tensor = torch.from_numpy(mask).unsqueeze(0).float()
        
        result = {
            'image': img_tensor,
            'mask': mask_tensor,
            'name': sample['base_name']
        }
        
        if bbox is not None:
            result['bbox'] = torch.tensor(bbox, dtype=torch.float32)
            
        return result


def collate_fn(batch: List[Dict]) -> Dict:
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    images = torch.stack([item['image'] for item in batch])
    masks = torch.stack([item['mask'] for item in batch])
    names = [item['name'] for item in batch]
    
    result = {
        'image': images,
        'mask': masks,
        'name': names
    }
    
    if 'bbox' in batch[0]:
        # bboxä¸èƒ½ç›´æ¥stackï¼Œä¿æŒä¸ºlist
        result['bbox'] = [item.get('bbox') for item in batch]
        
    return result


# ========================== è®­ç»ƒå™¨ ==========================

class Trainer:
    """å¾®è°ƒè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model,
        model_name: str,
        config: FinetuneConfig,
        train_loader: DataLoader,
        test_loader: Optional[DataLoader] = None,
        device: str = "cuda"
    ):
        self.model = model
        self.model_name = model_name
        self.config = config
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.device = device
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.output_dir, exist_ok=True)
        
        # è®¾ç½®å¾®è°ƒ
        self.model = setup_finetune(model, model_name, config)
        self.model.to(device)
        
        # æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = build_optimizer(model, config)
        self.scheduler = build_scheduler(
            self.optimizer, 
            config, 
            len(train_loader) // config.gradient_accumulation_steps
        )
        
        # æŸå¤±å‡½æ•° - ã€ä¿®å¤ã€‘è°ƒæ•´æƒé‡ï¼ŒDiceå æ›´å¤§æ¯”é‡
        self.criterion = SegmentationLoss(bce_weight=0.3, dice_weight=0.7)
        
        # æ··åˆç²¾åº¦
        self.scaler = torch.cuda.amp.GradScaler() if config.use_amp else None
        
        # è®­ç»ƒè®°å½•
        self.best_dice = 0.0
        self.history = {
            'train_loss': [],
            'test_loss': [],
            'test_dice': [],
            'test_iou': []
        }
        
    def train_epoch(self, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.config.num_epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            images = batch['image'].to(self.device)
            masks = batch['mask'].to(self.device)
            bboxes = batch.get('bbox', [None] * len(images))
            
            # ã€è°ƒè¯•ã€‘ç¬¬ä¸€ä¸ªbatchæ‰“å°æ•°æ®åˆ†å¸ƒ
            if batch_idx == 0 and epoch == 0:
                print(f"\n[DEBUG] æ•°æ®åˆ†å¸ƒæ£€æŸ¥:")
                print(f"  image: min={images.min():.4f}, max={images.max():.4f}, mean={images.mean():.4f}")
                print(f"  mask: min={masks.min():.4f}, max={masks.max():.4f}, mean={masks.mean():.4f}")
                print(f"  maskå‰æ™¯æ¯”ä¾‹: {(masks > 0.5).float().mean():.4f}")
            
            # å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(enabled=self.config.use_amp):
                batch_loss = 0.0
                
                for i in range(len(images)):
                    img = images[i:i+1]
                    mask = masks[i:i+1]
                    bbox = bboxes[i].tolist() if bboxes[i] is not None else None
                    
                    # æ¨¡å‹å‰å‘
                    if self.model.prompt_required and bbox is not None:
                        pred_mask, _ = self.model(img, bbox, mask)
                    else:
                        pred_mask, _ = self.model(img, None, mask)
                    
                    # ã€ä¿®å¤ã€‘å§‹ç»ˆä½¿ç”¨criterionè®¡ç®—lossï¼ˆBCE+Diceï¼‰ï¼Œå¿½ç•¥æ¨¡å‹è¿”å›çš„loss
                    loss = self.criterion(pred_mask, mask)
                    
                    batch_loss += loss
                    
                    # ã€è°ƒè¯•ã€‘ç¬¬ä¸€ä¸ªæ ·æœ¬æ‰“å°é¢„æµ‹åˆ†å¸ƒ
                    if batch_idx == 0 and epoch == 0 and i == 0:
                        print(f"  pred: min={pred_mask.min():.4f}, max={pred_mask.max():.4f}, mean={pred_mask.mean():.4f}")
                        print(f"  loss: {loss.item():.4f}")
                    
                batch_loss = batch_loss / len(images)
                batch_loss = batch_loss / self.config.gradient_accumulation_steps
            
            # åå‘ä¼ æ’­
            if self.scaler:
                self.scaler.scale(batch_loss).backward()
            else:
                batch_loss.backward()
            
            # æ¢¯åº¦æ›´æ–°
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config.max_grad_norm
                    )
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config.max_grad_norm
                    )
                    self.optimizer.step()
                    
                self.optimizer.zero_grad()
                self.scheduler.step()
            
            total_loss += batch_loss.item() * self.config.gradient_accumulation_steps
            num_batches += 1
            
            pbar.set_postfix({
                'loss': f"{total_loss/num_batches:.4f}",
                'lr': f"{self.optimizer.param_groups[0]['lr']:.2e}"
            })
            
        return total_loss / num_batches
    
    @torch.no_grad()
    def validate(self) -> Dict[str, float]:
        """æµ‹è¯•"""
        if self.test_loader is None:
            return {}
            
        self.model.eval()
        total_loss = 0.0
        total_dice = 0.0
        total_iou = 0.0
        num_samples = 0
        
        for batch in tqdm(self.test_loader, desc="Testing"):
            images = batch['image'].to(self.device)
            masks = batch['mask'].to(self.device)
            bboxes = batch.get('bbox', [None] * len(images))
            
            for i in range(len(images)):
                img = images[i:i+1]
                mask = masks[i:i+1]
                bbox = bboxes[i].tolist() if bboxes[i] is not None else None
                
                # å‰å‘ä¼ æ’­
                if self.model.prompt_required and bbox is not None:
                    pred_mask = self.model(img, bbox)
                else:
                    pred_mask = self.model(img, None)
                
                if isinstance(pred_mask, tuple):
                    pred_mask = pred_mask[0]
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(pred_mask, mask)
                total_loss += loss.item()
                
                # è®¡ç®—æŒ‡æ ‡
                pred_bin = (pred_mask > 0.5).float()
                mask_bin = (mask > 0.5).float()
                
                intersection = (pred_bin * mask_bin).sum()
                union = pred_bin.sum() + mask_bin.sum() - intersection
                
                iou = (intersection / (union + 1e-6)).item()
                dice = (2 * intersection / (pred_bin.sum() + mask_bin.sum() + 1e-6)).item()
                
                total_iou += iou
                total_dice += dice
                num_samples += 1
                
        return {
            'loss': total_loss / num_samples,
            'dice': total_dice / num_samples,
            'iou': total_iou / num_samples
        }
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ")
        print(f"   æ¨¡å‹: {self.model_name}")
        print(f"   ç­–ç•¥: {self.config.strategy}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"{'='*60}\n")
        
        for epoch in range(self.config.num_epochs):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            self.history['train_loss'].append(train_loss)
            
            # æµ‹è¯•
            if self.test_loader and (epoch + 1) % self.config.eval_interval == 0:
                test_metrics = self.validate()
                self.history['test_loss'].append(test_metrics.get('loss', 0))
                self.history['test_dice'].append(test_metrics.get('dice', 0))
                self.history['test_iou'].append(test_metrics.get('iou', 0))
                
                print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
                print(f"   Train Loss: {train_loss:.4f}")
                print(f"   Test Loss: {test_metrics['loss']:.4f}")
                print(f"   Test Dice: {test_metrics['dice']:.4f}")
                print(f"   Test IoU: {test_metrics['iou']:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if test_metrics['dice'] > self.best_dice:
                    self.best_dice = test_metrics['dice']
                    # ä¿å­˜ä¸º best_model.pth
                    best_path = os.path.join(self.config.output_dir, "best_model.pth")
                    checkpoint = {
                        "epoch": epoch + 1,
                        "model_state_dict": self.model.state_dict(),
                        "optimizer_state_dict": self.optimizer.state_dict(),
                        "config": self.config.to_dict(),
                        "metrics": test_metrics
                    }
                    torch.save(checkpoint, best_path)
                    print(f"   âœ… æ–°çš„æœ€ä½³æ¨¡å‹! Dice: {self.best_dice:.4f}")
                    print(f"   ğŸ’¾ å·²ä¿å­˜: {best_path}")
            else:
                print(f"\nğŸ“Š Epoch {epoch+1}: Train Loss = {train_loss:.4f}")
            
            # æ³¨é‡Šæ‰å®šæœŸä¿å­˜ï¼Œåªä¿ç•™best_model.pthèŠ‚çœç©ºé—´
            # if (epoch + 1) % self.config.save_interval == 0:
            #     save_finetuned_model(
            #         self.model,
            #         self.config,
            #         epoch + 1,
            #         self.optimizer
            #     )
                
        print(f"\n{'='*60}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³Dice: {self.best_dice:.4f}")
        print(f"{'='*60}\n")
        
        return self.history


# ========================== å‚æ•°è§£æ ==========================

def get_parser():
    parser = argparse.ArgumentParser(description="åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹å¾®è°ƒ")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model_name", type=str, default="medsam",
                        choices=list_available_models(),
                        help="æ¨¡å‹åç§°")
    parser.add_argument("--model_config", type=str, default="model_config.json",
                        help="æ¨¡å‹é…ç½®æ–‡ä»¶")
    
    # æ•°æ®é…ç½®
    parser.add_argument("--data_path", type=str, required=True,
                        help="æ•°æ®é›†è·¯å¾„ (åŒ…å«images, masks, bbox.json)")
    parser.add_argument("--img_dir", type=str, default=None,
                        help="å›¾åƒç›®å½• (é»˜è®¤: data_path/images)")
    parser.add_argument("--mask_dir", type=str, default=None,
                        help="æ©ç ç›®å½• (é»˜è®¤: data_path/masks)")
    parser.add_argument("--bbox_json", type=str, default=None,
                        help="bboxæ–‡ä»¶ (é»˜è®¤: data_path/bbox_coordinates.json)")
    parser.add_argument("--test_split", type=float, default=0.2,
                        help="æµ‹è¯•é›†æ¯”ä¾‹")
    
    # å¾®è°ƒç­–ç•¥
    parser.add_argument("--strategy", type=str, default="decoder_only",
                        choices=["decoder_only", "decoder_prompt", "adapter_only",
                                "encoder_partial", "encoder_full", "full", "lora",
                                "lora_plus_encoder", "lora_plus_decoder"],
                        help="å¾®è°ƒç­–ç•¥")
    parser.add_argument("--unfreeze_encoder_layers", type=int, default=2,
                        help="è§£å†»encoderæœ€åNå±‚ (encoder_partialç­–ç•¥)")
    
    # LoRAé…ç½®
    parser.add_argument("--lora_r", type=int, default=8, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16, help="LoRA alpha")
    
    # ã€æ–°å¢ã€‘LoRA+ é…ç½®
    parser.add_argument("--lora_plus_lr_ratio", type=float, default=16.0,
                        help="LoRA+ BçŸ©é˜µå­¦ä¹ ç‡å€æ•° (è®ºæ–‡æ¨è16)")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--num_epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="å­¦ä¹ ç‡")  # ã€ä¿®å¤ã€‘ä»1e-4æé«˜åˆ°5e-4
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--image_size", type=int, default=256, help="å›¾åƒå°ºå¯¸")
    parser.add_argument("--use_amp", action="store_true", help="ä½¿ç”¨æ··åˆç²¾åº¦")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1,
                        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--output_dir", type=str, default="./finetune_output",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, 
                        default="cuda" if torch.cuda.is_available() else "cpu",
                        help="è®¾å¤‡")
    
    # å…¶ä»–
    parser.add_argument("--resume", type=str, default=None,
                        help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    parser.add_argument("--recommend", action="store_true",
                        help="æ¨èå¾®è°ƒç­–ç•¥")
    
    return parser


# ========================== ä¸»å‡½æ•° ==========================

def main():
    args = get_parser().parse_args()
    
    # è®¾ç½®è·¯å¾„
    data_path = Path(args.data_path)
    img_dir = args.img_dir or str(data_path / "images")
    mask_dir = args.mask_dir or str(data_path / "masks") 
    bbox_json = args.bbox_json or str(data_path / "bbox_coordinates.json")
    
    # ç­–ç•¥æ¨è
    if args.recommend:
        # ç»Ÿè®¡æ•°æ®é‡
        num_images = len(list(Path(img_dir).glob("*")))
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0
        
        strategy = recommend_strategy(
            dataset_size=num_images,
            gpu_memory_gb=gpu_memory
        )
        print(f"\nä½¿ç”¨æ¨èç­–ç•¥: {strategy}")
        args.strategy = strategy
    
    # åˆ›å»ºé…ç½®
    config = FinetuneConfig(
        strategy=args.strategy,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        unfreeze_encoder_layers=args.unfreeze_encoder_layers,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_plus_lr_ratio=args.lora_plus_lr_ratio,  # ã€æ–°å¢ã€‘LoRA+ å‚æ•°
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        use_amp=args.use_amp,
        output_dir=args.output_dir
    )
    
    # åŠ è½½æ¨¡å‹é…ç½®
    model_cfg = {}
    if os.path.exists(args.model_config):
        with open(args.model_config, 'r') as f:
            all_cfg = json.load(f)
            model_cfg = all_cfg.get(args.model_name, {})
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {args.model_name}")
    model = create_model(args.model_name, args.device, model_cfg)
    
    # åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®é›†: {args.data_path}")
    
    full_dataset = SegmentationDataset(
        img_dir=img_dir,
        mask_dir=mask_dir,
        bbox_json=bbox_json,
        image_size=args.image_size,
        is_train=True,
        augmentation=True,
        model_name=args.model_name
    )
    
    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    test_size = int(len(full_dataset) * args.test_split)
    train_size = len(full_dataset) - test_size
    
    train_dataset, test_dataset = torch.utils.data.random_split(
        full_dataset, 
        [train_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # ä¿å­˜æµ‹è¯•é›†æ–‡ä»¶ååˆ—è¡¨ï¼ˆä¾› pipeline.py ä½¿ç”¨ï¼‰
    os.makedirs(config.output_dir, exist_ok=True)
    test_indices = test_dataset.indices
    test_samples_info = {
        "test_files": [full_dataset.samples[i]['base_name'] for i in test_indices],
        "train_files": [full_dataset.samples[i]['base_name'] for i in train_dataset.indices],
        "test_split": args.test_split,
        "random_seed": 42,
        "total_samples": len(full_dataset),
        "test_samples": test_size,
        "train_samples": train_size
    }
    test_samples_path = os.path.join(config.output_dir, "data_split.json")
    with open(test_samples_path, 'w') as f:
        json.dump(test_samples_info, f, indent=2)
    print(f"   ğŸ’¾ æ•°æ®åˆ’åˆ†å·²ä¿å­˜: {test_samples_path}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    print(f"   è®­ç»ƒæ ·æœ¬: {train_size}")
    print(f"   æµ‹è¯•æ ·æœ¬: {test_size}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        model_name=args.model_name,
        config=config,
        train_loader=train_loader,
        test_loader=test_loader,
        device=args.device
    )
    
    # æ¢å¤è®­ç»ƒ
    if args.resume:
        model, checkpoint = load_finetuned_model(model, args.resume)
        print(f"âœ… ä»æ£€æŸ¥ç‚¹æ¢å¤: {args.resume}")
    
    # å¼€å§‹è®­ç»ƒ
    history = trainer.train()
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_path = os.path.join(config.output_dir, "training_history.json")
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    print(f"âœ… è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


if __name__ == "__main__":
    main()
