#!/usr/bin/env python3
"""
å¾®è°ƒå·¥å…·æ¨¡å— - å®Œæ•´ä¿®å¤ç‰ˆ
åŒ…å«è¯¦ç»†è¯Šæ–­è¾“å‡ºï¼Œè¿è¡Œåå¯ä»¥çŸ¥é“é—®é¢˜åœ¨å“ªé‡Œ

ã€ä¿®å¤å†…å®¹ã€‘
1. warmup_epochs ä» 1 æ”¹ä¸º 0.1ï¼ˆå‡å°‘warmupæ—¶é—´ï¼‰
2. learning_rate é»˜è®¤å€¼ä» 1e-4 æ”¹ä¸º 5e-4
3. SegmentationLoss å¢åŠ æ•°å€¼ç¨³å®šæ€§å¤„ç†

ã€æ–°å¢å†…å®¹ - LoRA+ã€‘
4. æ·»åŠ  lora_plus_encoder å’Œ lora_plus_decoder ç­–ç•¥
5. LoRA+ å¯¹ A/B çŸ©é˜µä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum


class FinetuneStrategy(Enum):
    DECODER_ONLY = "decoder_only"
    DECODER_PROMPT = "decoder_prompt"
    ADAPTER_ONLY = "adapter_only"
    ENCODER_PARTIAL = "encoder_partial"
    ENCODER_FULL = "encoder_full"
    FULL = "full"
    LORA = "lora"
    # æ–°å¢ LoRA+ ç­–ç•¥
    LORA_PLUS_ENCODER = "lora_plus_encoder"  # LoRA+ åº”ç”¨äº encoderï¼ŒåŒæ—¶å¸¸è§„è®­ç»ƒ decoder
    LORA_PLUS_DECODER = "lora_plus_decoder"  # LoRA+ åº”ç”¨äº decoder

@dataclass
class FinetuneConfig:
    strategy: str = "decoder_only"
    learning_rate: float = 5e-4  # ã€ä¿®å¤ã€‘ä»1e-4æé«˜åˆ°5e-4
    weight_decay: float = 0.01
    num_epochs: int = 10
    batch_size: int = 4
    encoder_lr_scale: float = 0.1
    unfreeze_encoder_layers: int = 4  # é»˜è®¤è§£å†»4å±‚
    lora_r: int = 8  # ç»Ÿä¸€é»˜è®¤å€¼ï¼Œä¸ argparse ä¸€è‡´
    lora_alpha: int = 16
    lora_dropout: float = 0.1
    lora_target_modules: List[str] = field(default_factory=lambda: ["q_proj", "v_proj"])
    # ã€æ–°å¢ã€‘LoRA+ ç‰¹æœ‰å‚æ•°
    lora_plus_lr_ratio: float = 16.0  # BçŸ©é˜µå­¦ä¹ ç‡ = AçŸ©é˜µå­¦ä¹ ç‡ * ratio (LoRA+ è®ºæ–‡æ¨è 16)
    warmup_epochs: float = 0.1  # ã€ä¿®å¤ã€‘ä»1æ”¹ä¸º0.1ï¼Œåªwarmup 10%çš„ç¬¬ä¸€ä¸ªepoch
    save_interval: int = 1
    eval_interval: int = 1
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0
    use_amp: bool = True
    use_augmentation: bool = True
    augmentation_prob: float = 0.5
    output_dir: str = "./finetune_output"
    checkpoint_name: str = "finetuned_model.pth"
    
    def to_dict(self) -> Dict:
        return {
            "strategy": self.strategy,
            "learning_rate": self.learning_rate,
            "weight_decay": self.weight_decay,
            "num_epochs": self.num_epochs,
            "batch_size": self.batch_size,
            "encoder_lr_scale": self.encoder_lr_scale,
            "unfreeze_encoder_layers": self.unfreeze_encoder_layers,
            "lora_r": self.lora_r,
            "lora_alpha": self.lora_alpha,
            "lora_dropout": self.lora_dropout,
            "lora_target_modules": self.lora_target_modules,
            "lora_plus_lr_ratio": self.lora_plus_lr_ratio,  # ã€æ–°å¢ã€‘
            "warmup_epochs": self.warmup_epochs,
            "use_amp": self.use_amp,
            "output_dir": self.output_dir,
        }


def freeze_module(module: nn.Module):
    """å†»ç»“æ¨¡å—"""
    for param in module.parameters():
        param.requires_grad = False


def unfreeze_module(module: nn.Module):
    """è§£å†»æ¨¡å—"""
    for param in module.parameters():
        param.requires_grad = True


def count_parameters(model: nn.Module) -> Tuple[int, int]:
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return trainable, total


def print_trainable_parameters(model: nn.Module, name: str = "Model"):
    trainable, total = count_parameters(model)
    pct = 100 * trainable / total if total > 0 else 0
    print(f"\n{'='*70}")
    print(f"ğŸ“Š {name} å‚æ•°ç»Ÿè®¡:")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable:,}")
    print(f"   æ€»å‚æ•°:     {total:,}")
    print(f"   å¯è®­ç»ƒæ¯”ä¾‹: {pct:.2f}%")
    print(f"{'='*70}")


def diagnose_model_structure(model, model_name):
    """è¯Šæ–­æ¨¡å‹ç»“æ„ - å…³é”®è¯Šæ–­å‡½æ•°"""
    print(f"\n{'='*70}")
    print(f"ğŸ”¬ è¯Šæ–­ {model_name} æ¨¡å‹ç»“æ„")
    print(f"{'='*70}")
    
    # è·å–å†…éƒ¨SAMæ¨¡å‹
    if hasattr(model, 'model'):
        sam = model.model
    else:
        sam = model
    
    print(f"\nğŸ“¦ é¡¶å±‚æ¨¡å—:")
    for name, child in sam.named_children():
        param_count = sum(p.numel() for p in child.parameters())
        print(f"   {name}: {type(child).__name__} ({param_count:,} å‚æ•°)")
    
    # æ£€æŸ¥encoderç»“æ„
    if hasattr(sam, 'vision_encoder'):
        encoder = sam.vision_encoder
        print(f"\nğŸ“¦ vision_encoder ç»“æ„:")
        for name, child in encoder.named_children():
            if isinstance(child, nn.ModuleList):
                print(f"   {name}: ModuleList (é•¿åº¦={len(child)})")
            else:
                print(f"   {name}: {type(child).__name__}")
        
        # æ£€æŸ¥layerså±æ€§
        print(f"\nğŸ” encoderå±‚å±æ€§æ£€æŸ¥:")
        for attr in ['layers', 'blocks', 'layer', 'encoder_layers']:
            if hasattr(encoder, attr):
                obj = getattr(encoder, attr)
                if isinstance(obj, nn.ModuleList):
                    print(f"   âœ… encoder.{attr} å­˜åœ¨, é•¿åº¦={len(obj)}")
                else:
                    print(f"   âš ï¸ encoder.{attr} å­˜åœ¨ä½†ä¸æ˜¯ModuleList: {type(obj)}")
            else:
                print(f"   âŒ encoder.{attr} ä¸å­˜åœ¨")


def diagnose_trainable_params(model, strategy):
    """è¯Šæ–­å¯è®­ç»ƒå‚æ•°åˆ†å¸ƒ"""
    print(f"\n{'='*70}")
    print(f"ğŸ”¬ è¯Šæ–­å¯è®­ç»ƒå‚æ•° (ç­–ç•¥: {strategy})")
    print(f"{'='*70}")
    
    if hasattr(model, 'model'):
        sam = model.model
    else:
        sam = model
    
    # æŒ‰æ¨¡å—ç»Ÿè®¡
    module_stats = {}
    for name, param in sam.named_parameters():
        parts = name.split('.')
        module = parts[0] if parts else 'other'
        
        if module not in module_stats:
            module_stats[module] = {'total': 0, 'trainable': 0}
        
        module_stats[module]['total'] += param.numel()
        if param.requires_grad:
            module_stats[module]['trainable'] += param.numel()
    
    print(f"\nğŸ“Š å„æ¨¡å—å¯è®­ç»ƒå‚æ•°:")
    for module, stats in sorted(module_stats.items()):
        pct = 100 * stats['trainable'] / stats['total'] if stats['total'] > 0 else 0
        status = "âœ…" if stats['trainable'] > 0 else "âŒ"
        print(f"   {status} {module}: {stats['trainable']:,} / {stats['total']:,} ({pct:.1f}%)")
    
    # éªŒè¯ç­–ç•¥æ˜¯å¦æ­£ç¡®æ‰§è¡Œ
    print(f"\nğŸ” ç­–ç•¥éªŒè¯:")
    
    if strategy == "decoder_only":
        if module_stats.get('mask_decoder', {}).get('trainable', 0) == 0:
            print(f"   âŒ é”™è¯¯: mask_decoder åº”è¯¥å¯è®­ç»ƒä½†å®é™…ä¸æ˜¯!")
        else:
            print(f"   âœ… mask_decoder å·²è§£å†»")
        if module_stats.get('vision_encoder', {}).get('trainable', 0) > 0:
            print(f"   âš ï¸ è­¦å‘Š: vision_encoder ä¸åº”è¯¥è¢«è®­ç»ƒ!")
            
    elif strategy == "decoder_prompt":
        if module_stats.get('mask_decoder', {}).get('trainable', 0) == 0:
            print(f"   âŒ é”™è¯¯: mask_decoder åº”è¯¥å¯è®­ç»ƒ!")
        else:
            print(f"   âœ… mask_decoder å·²è§£å†»")
        if module_stats.get('prompt_encoder', {}).get('trainable', 0) == 0:
            print(f"   âŒ é”™è¯¯: prompt_encoder åº”è¯¥å¯è®­ç»ƒ!")
        else:
            print(f"   âœ… prompt_encoder å·²è§£å†»")
            
    elif strategy == "encoder_partial":
        if module_stats.get('vision_encoder', {}).get('trainable', 0) == 0:
            print(f"   âŒ é”™è¯¯: vision_encoder åº”è¯¥éƒ¨åˆ†å¯è®­ç»ƒ!")
            print(f"   âš ï¸ encoder_partial å®é™…ç­‰åŒäº decoder_prompt!")
        else:
            print(f"   âœ… vision_encoder éƒ¨åˆ†å·²è§£å†»")

    elif strategy == "encoder_full":
        if module_stats.get('vision_encoder', {}).get('trainable', 0) == 0:
            print(f"   âŒ é”™è¯¯: vision_encoder åº”è¯¥å…¨éƒ¨å¯è®­ç»ƒ!")
        else:
            print(f"   âœ… vision_encoder å·²å…¨éƒ¨è§£å†»")

    elif strategy == "full":
        total_trainable = sum(s['trainable'] for s in module_stats.values())
        total_params = sum(s['total'] for s in module_stats.values())
        if total_trainable < total_params * 0.99:
            print(f"   âš ï¸ è­¦å‘Š: fullç­–ç•¥ä½†ä¸æ˜¯æ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒ")
        else:
            print(f"   âœ… æ‰€æœ‰å‚æ•°å·²è§£å†»")
            
    elif strategy == "lora":
        # æ£€æŸ¥æ˜¯å¦æœ‰loraå‚æ•°
        lora_params = sum(1 for n, p in sam.named_parameters() if 'lora' in n.lower() and p.requires_grad)
        if lora_params == 0:
            print(f"   âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°å¯è®­ç»ƒçš„LoRAå‚æ•°!")
        else:
            print(f"   âœ… æ‰¾åˆ° {lora_params} ä¸ªLoRAå‚æ•°")
    
    # ã€æ–°å¢ã€‘LoRA+ ç­–ç•¥éªŒè¯
    elif strategy in ["lora_plus_encoder", "lora_plus_decoder"]:
        lora_params = sum(1 for n, p in sam.named_parameters() if 'lora' in n.lower() and p.requires_grad)
        if lora_params == 0:
            print(f"   âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°å¯è®­ç»ƒçš„LoRAå‚æ•°!")
        else:
            print(f"   âœ… æ‰¾åˆ° {lora_params} ä¸ªLoRAå‚æ•°")
        
        # æ£€æŸ¥ decoder æ˜¯å¦è¢«è§£å†»ï¼ˆå¯¹äº lora_plus_encoderï¼‰
        if strategy == "lora_plus_encoder":
            if module_stats.get('mask_decoder', {}).get('trainable', 0) == 0:
                print(f"   âš ï¸ è­¦å‘Š: mask_decoder æœªè¢«è§£å†»")
            else:
                print(f"   âœ… mask_decoder å·²è§£å†»ï¼ˆè”åˆè®­ç»ƒï¼‰")
    
    return module_stats


# ========================== LoRA+ å®ç° ==========================

def get_lora_target_modules_for_model(model_name: str, component: str = "encoder") -> List[str]:
    """
    æ ¹æ®æ¨¡å‹ç±»å‹è¿”å›æ­£ç¡®çš„ LoRA target_modules

    Args:
        model_name: æ¨¡å‹åç§° ('medsam' æˆ– 'sammed2d')
        component: 'encoder' æˆ– 'decoder'

    Returns:
        target_modules åˆ—è¡¨

    æ³¨æ„ - SAM-Med2D qkv å±‚çš„ç‰¹æ®Šæ€§:
        SAM-Med2D encoder çš„ qkv å±‚æ˜¯ nn.Linear(dim, dim * 3)ï¼Œè¾“å‡ºç»´åº¦æ˜¯è¾“å…¥çš„ 3 å€ï¼ˆQ/K/V åˆå¹¶ï¼‰ã€‚
        å¯¹ qkv å±‚åŠ  LoRA ä¼šåŒæ—¶è°ƒæ•´ Qã€Kã€V ä¸‰ä¸ªæŠ•å½±ï¼Œæ— æ³•åƒ MedSAM é‚£æ ·åªè°ƒ Q å’Œ V è€Œä¸è°ƒ Kã€‚
        è¿™æ„å‘³ç€:
        1. å‚æ•°é‡æ¯”åˆ†å¼€çš„ q_proj/v_proj çº¦å¤š 1.5 å€ï¼ˆå› ä¸ºä¹ŸåŒ…å«äº† K çš„è°ƒæ•´ï¼‰
        2. è¯­ä¹‰ä¸Šæ˜¯å¯¹æ•´ä¸ª attention æŠ•å½±åš low-rank è°ƒæ•´ï¼Œè€Œéä»…è°ƒ Q/V
    """
    if model_name.lower() == "sammed2d":
        # SAM-Med2D ä½¿ç”¨ segment_anything åº“
        # ViT encoder çš„ Attention ä½¿ç”¨åˆå¹¶çš„ qkv å±‚: self.qkv = nn.Linear(dim, dim * 3)
        # mask_decoder çš„ Attention ä½¿ç”¨: self.q_proj, self.k_proj, self.v_proj
        if component == "encoder":
            return ["qkv", "proj"]  # encoder ä½¿ç”¨åˆå¹¶çš„ qkv
        else:
            return ["q_proj", "v_proj", "out_proj"]  # decoder ä½¿ç”¨åˆ†å¼€çš„
    else:
        # MedSAM (HuggingFace transformers.SamModel)
        # vision_encoder å’Œ mask_decoder éƒ½ä½¿ç”¨åˆ†å¼€çš„ q_proj, v_proj
        return ["q_proj", "v_proj"]


class LoRAPlusLinear(nn.Module):
    """
    LoRA+ Linear å±‚å®ç°
    æ ¸å¿ƒåˆ›æ–°: AçŸ©é˜µå’ŒBçŸ©é˜µä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
    BçŸ©é˜µå­¦ä¹ ç‡ = AçŸ©é˜µå­¦ä¹ ç‡ * lr_ratio (è®ºæ–‡æ¨è lr_ratio=16)
    """
    def __init__(
        self, 
        original_linear: nn.Linear, 
        r: int = 8, 
        alpha: int = 16, 
        dropout: float = 0.1
    ):
        super().__init__()
        self.original_linear = original_linear
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r
        
        in_features = original_linear.in_features
        out_features = original_linear.out_features
        
        # å†»ç»“åŸå§‹æƒé‡
        original_linear.weight.requires_grad = False
        if original_linear.bias is not None:
            original_linear.bias.requires_grad = False
        
        # LoRA çŸ©é˜µ - è¿™ä¸¤ä¸ªä¼šè¢«åˆ†é…ä¸åŒçš„å­¦ä¹ ç‡
        # lora_A: (r, in_features) - ä½¿ç”¨åŸºç¡€å­¦ä¹ ç‡
        # lora_B: (out_features, r) - ä½¿ç”¨ lr * lr_ratio
        self.lora_A = nn.Parameter(torch.zeros(r, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, r))
        
        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A, a=5**0.5)
        nn.init.zeros_(self.lora_B)  # Båˆå§‹åŒ–ä¸º0ï¼Œä¿è¯åˆå§‹è¾“å‡ºä¸å˜
        
        # Dropout
        self.lora_dropout = nn.Dropout(p=dropout) if dropout > 0 else nn.Identity()
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # åŸå§‹è¾“å‡º
        result = self.original_linear(x)
        
        # LoRA å¢é‡: (x @ A^T) @ B^T * scaling
        lora_out = self.lora_dropout(x)
        lora_out = lora_out @ self.lora_A.T  # (batch, seq, r)
        lora_out = lora_out @ self.lora_B.T  # (batch, seq, out)
        lora_out = lora_out * self.scaling
        
        return result + lora_out


def _get_submodule(model, target_name: str):
    """æ ¹æ®ç‚¹åˆ†éš”çš„åç§°è·å–å­æ¨¡å—"""
    parts = target_name.split('.')
    module = model
    for part in parts:
        module = getattr(module, part)
    return module


def _set_submodule(model, target_name: str, new_module: nn.Module):
    """è®¾ç½®å­æ¨¡å—"""
    parts = target_name.split('.')
    parent = model
    for part in parts[:-1]:
        parent = getattr(parent, part)
    setattr(parent, parts[-1], new_module)


def _setup_lora_plus(model, config: FinetuneConfig, component: str, model_name: str):
    """
    è®¾ç½® LoRA+ å¾®è°ƒ

    Args:
        model: æ¨¡å‹åŒ…è£…å™¨
        config: å¾®è°ƒé…ç½®
        component: 'encoder' æˆ– 'decoder'
        model_name: æ¨¡å‹åç§°ï¼Œç”¨äºè‡ªåŠ¨é€‰æ‹© target_modules
    """
    if hasattr(model, 'model'):
        sam = model.model
    else:
        sam = model

    # ã€ä¿®å¤ã€‘æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨é€‰æ‹© target_modules
    target_modules = get_lora_target_modules_for_model(model_name, component)

    print(f"\n{'='*70}")
    print(f"ğŸ”§ è®¾ç½® LoRA+ ({component}) - æ¨¡å‹: {model_name}")
    print(f"   r={config.lora_r}, alpha={config.lora_alpha}")
    print(f"   lr_ratio={config.lora_plus_lr_ratio} (BçŸ©é˜µå­¦ä¹ ç‡å€æ•°)")
    print(f"   target_modules={target_modules} (è‡ªåŠ¨æ£€æµ‹)")
    print(f"{'='*70}")
    
    # ç¡®å®šæœç´¢èŒƒå›´
    if component == "encoder":
        if hasattr(sam, 'vision_encoder'):
            search_root = sam.vision_encoder
            root_name = 'vision_encoder'
        elif hasattr(sam, 'image_encoder'):
            search_root = sam.image_encoder
            root_name = 'image_encoder'
        else:
            print(f"   âŒ é”™è¯¯: æ‰¾ä¸åˆ° encoder æ¨¡å—!")
            return
        search_roots = [(search_root, root_name)]
    elif component == "decoder":
        # ã€ä¿®å¤ã€‘åªåœ¨ mask_decoder ä¸­æœç´¢ï¼Œprompt_encoder åŸºæœ¬æ²¡æœ‰ attention å±‚
        search_roots = []
        if hasattr(sam, 'mask_decoder'):
            search_roots.append((sam.mask_decoder, 'mask_decoder'))
        # ç§»é™¤ prompt_encoder æœç´¢ï¼ˆå®ƒä¸»è¦æ˜¯ Embedding å±‚ï¼Œä¸ä¼šæœ‰ q_proj/v_projï¼‰
    else:
        raise ValueError(f"Unknown component: {component}")
    
    applied_count = 0
    
    for search_root, root_name in search_roots:
        for name, module in search_root.named_modules():
            if isinstance(module, nn.Linear):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¨¡å—
                full_name = f"{root_name}.{name}" if name else root_name
                is_target = False
                for target in target_modules:
                    if target in name or name.endswith(target):
                        is_target = True
                        break
                
                if is_target:
                    # åˆ›å»º LoRA+ æ¨¡å—
                    lora_module = LoRAPlusLinear(
                        original_linear=module,
                        r=config.lora_r,
                        alpha=config.lora_alpha,
                        dropout=config.lora_dropout
                    )
                    
                    # æ›¿æ¢åŸæ¨¡å—
                    _set_submodule(search_root, name, lora_module)
                    
                    applied_count += 1
                    print(f"      âœ… {full_name}")
    
    if applied_count == 0:
        print(f"   âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡æ¨¡å—!")
        print(f"   æ£€æŸ¥ target_modules: {target_modules}")
    else:
        print(f"\n   âœ… LoRA+ å·²åº”ç”¨åˆ° {applied_count} ä¸ªæ¨¡å—")
    
    # ç»Ÿè®¡ LoRA å‚æ•°
    lora_a_params = 0
    lora_b_params = 0
    for name, param in sam.named_parameters():
        if 'lora_A' in name and param.requires_grad:
            lora_a_params += param.numel()
        elif 'lora_B' in name and param.requires_grad:
            lora_b_params += param.numel()
    
    print(f"\n   ğŸ“Š LoRA+ å‚æ•°ç»Ÿè®¡:")
    print(f"      lora_A å‚æ•°: {lora_a_params:,}")
    print(f"      lora_B å‚æ•°: {lora_b_params:,}")
    print(f"      æ€» LoRA å‚æ•°: {lora_a_params + lora_b_params:,}")


# ========================== MedSAM å¾®è°ƒ ==========================

def setup_medsam_finetune(model, config: FinetuneConfig):
    """è®¾ç½®MedSAMå¾®è°ƒ - å¸¦å®Œæ•´è¯Šæ–­"""
    strategy = config.strategy
    sam = model.model  # transformers.SamModel
    
    print(f"\n{'='*70}")
    print(f"ğŸ”§ è®¾ç½®MedSAMå¾®è°ƒ")
    print(f"   ç­–ç•¥: {strategy}")
    print(f"   unfreeze_encoder_layers: {config.unfreeze_encoder_layers}")
    print(f"{'='*70}")
    
    # è¯Šæ–­æ¨¡å‹ç»“æ„
    diagnose_model_structure(model, "MedSAM")
    
    # Step 1: å†»ç»“æ‰€æœ‰å‚æ•°
    print(f"\n[Step 1] å†»ç»“æ‰€æœ‰å‚æ•°...")
    freeze_module(sam)
    t1, _ = count_parameters(sam)
    print(f"   å†»ç»“åå¯è®­ç»ƒå‚æ•°: {t1:,}")
    
    # Step 2: æ ¹æ®ç­–ç•¥è§£å†»
    print(f"\n[Step 2] æ ¹æ®ç­–ç•¥ '{strategy}' è§£å†»å‚æ•°...")
    
    if strategy == "decoder_only":
        unfreeze_module(sam.mask_decoder)
        print(f"   è§£å†»: mask_decoder")
        
    elif strategy == "decoder_prompt":
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        print(f"   è§£å†»: mask_decoder, prompt_encoder")
        
    elif strategy == "encoder_partial":
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        print(f"   è§£å†»: mask_decoder, prompt_encoder")
        
        # è§£å†»encoderæœ€åNå±‚
        n_layers = config.unfreeze_encoder_layers
        encoder = sam.vision_encoder
        
        # æŸ¥æ‰¾encoderçš„layers
        layers = None
        layer_attr = None
        for attr in ['layers', 'blocks', 'layer', 'encoder_layers']:
            if hasattr(encoder, attr):
                candidate = getattr(encoder, attr)
                if isinstance(candidate, nn.ModuleList) and len(candidate) > 0:
                    layers = candidate
                    layer_attr = attr
                    break
        
        if layers is not None:
            total = len(layers)
            if n_layers > 0:
                start_idx = max(0, total - n_layers)
                for i in range(start_idx, total):
                    unfreeze_module(layers[i])
                    print(f"   è§£å†»: encoder.{layer_attr}[{i}]")
                print(f"   âœ… å…±è§£å†»encoder {n_layers}/{total} å±‚")
            else:
                print(f"   âš ï¸ n_layers=0, encoderä¸ä¼šè¢«è§£å†»!")
        else:
            print(f"   âŒ é”™è¯¯: æ‰¾ä¸åˆ°encoderçš„layers!")
            print(f"   âš ï¸ encoder_partial å°†ç­‰åŒäº decoder_prompt!")

    elif strategy == "encoder_full":
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        unfreeze_module(sam.vision_encoder)
        print(f"   è§£å†»: mask_decoder, prompt_encoder, vision_encoder (å…¨éƒ¨)")

    elif strategy == "full":
        unfreeze_module(sam)
        print(f"   è§£å†»: å…¨éƒ¨å‚æ•°")
        
    elif strategy == "lora":
        print(f"   åº”ç”¨LoRA...")
        _setup_lora_medsam(model, config)

    # ã€æ–°å¢ã€‘LoRA+ encoder ç­–ç•¥
    elif strategy == "lora_plus_encoder":
        print(f"   åº”ç”¨ LoRA+ (encoder)...")
        _setup_lora_plus(model, config, component="encoder", model_name="medsam")
        # åŒæ—¶è§£å†» decoder è¿›è¡Œå¸¸è§„è®­ç»ƒ
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        print(f"   âœ… åŒæ—¶è§£å†» decoder å’Œ prompt_encoder è¿›è¡Œå¸¸è§„è®­ç»ƒ")

    # ã€æ–°å¢ã€‘LoRA+ decoder ç­–ç•¥
    elif strategy == "lora_plus_decoder":
        print(f"   åº”ç”¨ LoRA+ (decoder)...")
        _setup_lora_plus(model, config, component="decoder", model_name="medsam")
        # ã€ä¿®å¤ã€‘ä¹Ÿè§£å†» prompt_encoderï¼Œå¢åŠ å¯è®­ç»ƒå‚æ•°
        unfreeze_module(sam.prompt_encoder)
        print(f"   âœ… åŒæ—¶è§£å†» prompt_encoder è¿›è¡Œå¸¸è§„è®­ç»ƒ")

    else:
        raise ValueError(f"æœªçŸ¥ç­–ç•¥: {strategy}")

    # Step 3: è¯Šæ–­ç»“æœ
    print(f"\n[Step 3] éªŒè¯è§£å†»ç»“æœ...")
    diagnose_trainable_params(model, strategy)

    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print_trainable_parameters(sam, "MedSAM")
    
    return model


def _setup_lora_medsam(model, config: FinetuneConfig):
    """ä¸ºMedSAMè®¾ç½®LoRA"""
    sam = model.model
    
    # âœ… æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯PEFTæ¨¡å‹ï¼Œé¿å…é‡å¤åŒ…è£…
    try:
        from peft import PeftModel
        if isinstance(sam, PeftModel):
            print(f"   âš ï¸ æ¨¡å‹å·²ç»æ˜¯PEFTæ¨¡å‹ï¼Œè·³è¿‡é‡å¤åŒ…è£…")
            return
    except ImportError:
        pass
    
    try:
        from peft import get_peft_model, LoraConfig
        
        print(f"   ä½¿ç”¨peftåº“è®¾ç½®LoRA...")
        print(f"   r={config.lora_r}, alpha={config.lora_alpha}")
        print(f"   target_modules={config.lora_target_modules}")
        
        lora_config = LoraConfig(
            r=config.lora_r,
            lora_alpha=config.lora_alpha,
            target_modules=config.lora_target_modules,
            lora_dropout=config.lora_dropout,
            bias="none",
        )
        
        # åº”ç”¨LoRA - è¿™ä¼šä¿®æ”¹æ¨¡å‹ç»“æ„
        model.model = get_peft_model(sam, lora_config)
        print(f"   âœ… LoRAé…ç½®æˆåŠŸ")
        
        # æ‰“å°LoRAå‚æ•°
        lora_params = [(n, p.numel()) for n, p in model.model.named_parameters() 
                       if 'lora' in n.lower() and p.requires_grad]
        print(f"   LoRAå‚æ•°æ•°é‡: {len(lora_params)}")
        for name, count in lora_params[:5]:
            print(f"      - {name}: {count:,}")
        if len(lora_params) > 5:
            print(f"      ... åŠå…¶ä»– {len(lora_params)-5} ä¸ª")
            
    except ImportError as e:
        print(f"   âš ï¸ peftåº“æœªå®‰è£…: {e}")
        print(f"   ä½¿ç”¨æ‰‹åŠ¨LoRAå®ç°...")
        _apply_manual_lora(sam, config)
    except Exception as e:
        print(f"   âŒ LoRAè®¾ç½®å¤±è´¥: {e}")
        raise


def _apply_manual_lora(model, config):
    """æ‰‹åŠ¨LoRAå®ç°"""
    count = 0
    
    # âœ… è·å–æ¨¡å‹æ‰€åœ¨è®¾å¤‡
    device = next(model.parameters()).device
    print(f"   æ¨¡å‹è®¾å¤‡: {device}")
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            for target in config.lora_target_modules:
                if target in name:
                    in_f, out_f = module.in_features, module.out_features
                    
                    # å†»ç»“åŸå§‹æƒé‡
                    module.weight.requires_grad = False
                    if module.bias is not None:
                        module.bias.requires_grad = False
                    
                    # âœ… æ·»åŠ LoRAå‚æ•° - åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šåˆ›å»º
                    lora_A = nn.Parameter(torch.zeros(config.lora_r, in_f, device=device))
                    lora_B = nn.Parameter(torch.zeros(out_f, config.lora_r, device=device))
                    nn.init.kaiming_uniform_(lora_A, a=5**0.5)
                    nn.init.zeros_(lora_B)
                    
                    module.register_parameter('lora_A', lora_A)
                    module.register_parameter('lora_B', lora_B)
                    module.scaling = config.lora_alpha / config.lora_r
                    
                    # ä¿®æ”¹forward - âœ… ç¡®ä¿è®¾å¤‡ä¸€è‡´
                    original_forward = module.forward
                    def new_forward(x, orig_fwd=original_forward, mod=module):
                        result = orig_fwd(x)
                        # âœ… ç¡®ä¿LoRAå‚æ•°åœ¨è¾“å…¥ç›¸åŒçš„è®¾å¤‡ä¸Š
                        lora_A = mod.lora_A.to(x.device)
                        lora_B = mod.lora_B.to(x.device)
                        lora_out = (x @ lora_A.T @ lora_B.T) * mod.scaling
                        return result + lora_out
                    module.forward = new_forward
                    
                    count += 1
                    print(f"      åº”ç”¨LoRA: {name}")
                    break
    
    print(f"   æ‰‹åŠ¨LoRAåº”ç”¨åˆ° {count} ä¸ªå±‚")


# ========================== SAM-Med2D å¾®è°ƒ ==========================

def setup_sammed2d_finetune(model, config: FinetuneConfig):
    """è®¾ç½®SAM-Med2Då¾®è°ƒ"""
    strategy = config.strategy
    sam = model.model
    
    print(f"\n{'='*70}")
    print(f"ğŸ”§ è®¾ç½®SAM-Med2Då¾®è°ƒ")
    print(f"   ç­–ç•¥: {strategy}")
    print(f"{'='*70}")
    
    diagnose_model_structure(model, "SAM-Med2D")
    
    freeze_module(sam)
    
    if strategy == "decoder_only":
        unfreeze_module(sam.mask_decoder)
        
    elif strategy == "decoder_prompt":
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        
    elif strategy == "adapter_only":
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        
        adapter_count = 0
        for name, param in sam.image_encoder.named_parameters():
            if 'adapter' in name.lower() or 'Adapter' in name:
                param.requires_grad = True
                adapter_count += 1
        print(f"   è§£å†» {adapter_count} ä¸ªadapterå‚æ•°")
        
    elif strategy == "encoder_partial":
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        
        n_layers = config.unfreeze_encoder_layers
        if hasattr(sam.image_encoder, 'blocks') and n_layers > 0:
            total = len(sam.image_encoder.blocks)
            for i in range(total - n_layers, total):
                unfreeze_module(sam.image_encoder.blocks[i])
        
        for name, param in sam.image_encoder.named_parameters():
            if 'adapter' in name.lower():
                param.requires_grad = True

    elif strategy == "encoder_full":
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        unfreeze_module(sam.image_encoder)
        print(f"   è§£å†»: mask_decoder, prompt_encoder, image_encoder (å…¨éƒ¨)")

    elif strategy == "full":
        unfreeze_module(sam)

    elif strategy == "lora":
        # ã€ä¿®å¤ã€‘SAM-Med2D éœ€è¦ä½¿ç”¨ä¸åŒçš„ target_modules
        config.lora_target_modules = get_lora_target_modules_for_model("sammed2d", "encoder")
        print(f"   SAM-Med2D LoRA target_modules: {config.lora_target_modules}")
        _setup_lora_medsam(model, config)

    # ã€æ–°å¢ã€‘LoRA+ encoder ç­–ç•¥
    elif strategy == "lora_plus_encoder":
        print(f"   åº”ç”¨ LoRA+ (encoder)...")
        _setup_lora_plus(model, config, component="encoder", model_name="sammed2d")
        # åŒæ—¶è§£å†» decoder è¿›è¡Œå¸¸è§„è®­ç»ƒ
        unfreeze_module(sam.mask_decoder)
        unfreeze_module(sam.prompt_encoder)
        print(f"   âœ… åŒæ—¶è§£å†» decoder å’Œ prompt_encoder è¿›è¡Œå¸¸è§„è®­ç»ƒ")

    # ã€æ–°å¢ã€‘LoRA+ decoder ç­–ç•¥
    elif strategy == "lora_plus_decoder":
        print(f"   åº”ç”¨ LoRA+ (decoder)...")
        _setup_lora_plus(model, config, component="decoder", model_name="sammed2d")
        # ã€ä¿®å¤ã€‘ä¹Ÿè§£å†» prompt_encoderï¼Œå¢åŠ å¯è®­ç»ƒå‚æ•°
        unfreeze_module(sam.prompt_encoder)
        print(f"   âœ… åŒæ—¶è§£å†» prompt_encoder è¿›è¡Œå¸¸è§„è®­ç»ƒ")

    diagnose_trainable_params(model, strategy)
    print_trainable_parameters(sam, "SAM-Med2D")
    
    return model


# ========================== å…¥å£å‡½æ•° ==========================

def setup_finetune(model, model_name: str, config: FinetuneConfig):
    """ç»Ÿä¸€å…¥å£"""
    print(f"\n{'#'*70}")
    print(f"# å¾®è°ƒè®¾ç½®å¼€å§‹")
    print(f"# æ¨¡å‹: {model_name}")
    print(f"# ç­–ç•¥: {config.strategy}")
    print(f"# å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"# warmup_epochs: {config.warmup_epochs}")
    print(f"# unfreeze_encoder_layers: {config.unfreeze_encoder_layers}")
    # ã€æ–°å¢ã€‘æ‰“å° LoRA+ å‚æ•°
    if config.strategy.startswith("lora"):
        print(f"# LoRA r: {config.lora_r}")
        print(f"# LoRA alpha: {config.lora_alpha}")
    if config.strategy.startswith("lora_plus"):
        print(f"# LoRA+ lr_ratio: {config.lora_plus_lr_ratio}")
    print(f"{'#'*70}")
    
    if model_name.lower() == "medsam":
        result = setup_medsam_finetune(model, config)
    elif model_name.lower() == "sammed2d":
        result = setup_sammed2d_finetune(model, config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
    
    print(f"\n{'#'*70}")
    print(f"# å¾®è°ƒè®¾ç½®å®Œæˆ")
    print(f"{'#'*70}\n")
    
    return result


# ========================== ä¼˜åŒ–å™¨ ==========================

def build_optimizer(model, config: FinetuneConfig) -> torch.optim.Optimizer:
    """
    æ„å»ºä¼˜åŒ–å™¨
    
    ã€æ–°å¢ã€‘å¯¹äº LoRA+ ç­–ç•¥ï¼ŒAçŸ©é˜µå’ŒBçŸ©é˜µä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡:
    - lora_A: åŸºç¡€å­¦ä¹ ç‡ lr
    - lora_B: lr * lora_plus_lr_ratio (è®ºæ–‡æ¨è 16x)
    """
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    
    print(f"\nğŸ“Š æ„å»ºä¼˜åŒ–å™¨:")
    print(f"   å¯è®­ç»ƒå‚æ•°tensoræ•°: {len(trainable_params)}")
    
    if len(trainable_params) == 0:
        print(f"   âŒ é”™è¯¯: æ²¡æœ‰å¯è®­ç»ƒå‚æ•°!")
        raise RuntimeError("æ²¡æœ‰å¯è®­ç»ƒå‚æ•°! æ£€æŸ¥freeze/unfreezeé€»è¾‘")
    
    total_params = sum(p.numel() for p in trainable_params)
    print(f"   å¯è®­ç»ƒå‚æ•°æ€»é‡: {total_params:,}")
    
    # ã€æ–°å¢ã€‘LoRA+ æ¨¡å¼: åˆ†ç¦» A å’Œ B çŸ©é˜µçš„å‚æ•°ç»„
    if config.strategy.startswith("lora_plus"):
        lora_a_params = []
        lora_b_params = []
        other_params = []
        
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue
            
            if 'lora_A' in name:
                lora_a_params.append(param)
            elif 'lora_B' in name:
                lora_b_params.append(param)
            else:
                other_params.append(param)
        
        param_groups = []
        
        if lora_a_params:
            param_groups.append({
                "params": lora_a_params,
                "lr": config.learning_rate,
                "name": "lora_A"
            })
            print(f"   lora_Aç»„: {len(lora_a_params)} tensors, lr={config.learning_rate}")
        
        if lora_b_params:
            lora_b_lr = config.learning_rate * config.lora_plus_lr_ratio
            param_groups.append({
                "params": lora_b_params,
                "lr": lora_b_lr,
                "name": "lora_B"
            })
            print(f"   lora_Bç»„: {len(lora_b_params)} tensors, lr={lora_b_lr} ({config.lora_plus_lr_ratio}x)")
        
        if other_params:
            param_groups.append({
                "params": other_params,
                "lr": config.learning_rate,
                "name": "other"
            })
            print(f"   otherç»„: {len(other_params)} tensors, lr={config.learning_rate}")
        
        if len(param_groups) == 0:
            raise RuntimeError("æ²¡æœ‰å¯è®­ç»ƒå‚æ•°! æ£€æŸ¥freeze/unfreezeé€»è¾‘")
    
    # åˆ†å±‚å­¦ä¹ ç‡ (åŸæœ‰é€»è¾‘)
    elif config.strategy in ["encoder_partial", "encoder_full", "full"]:
        encoder_params = []
        other_params = []
        
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue
            if "encoder" in name.lower():
                encoder_params.append(param)
            else:
                other_params.append(param)
        
        param_groups = []
        if encoder_params:
            param_groups.append({
                "params": encoder_params,
                "lr": config.learning_rate * config.encoder_lr_scale,
                "name": "encoder"
            })
            print(f"   encoderç»„: {len(encoder_params)} tensors, lr={config.learning_rate * config.encoder_lr_scale}")
        if other_params:
            param_groups.append({
                "params": other_params,
                "lr": config.learning_rate,
                "name": "other"
            })
            print(f"   otherç»„: {len(other_params)} tensors, lr={config.learning_rate}")
    else:
        param_groups = [{"params": trainable_params, "lr": config.learning_rate}]
        print(f"   ç»Ÿä¸€å­¦ä¹ ç‡: {config.learning_rate}")
    
    optimizer = torch.optim.AdamW(
        param_groups,
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )
    
    return optimizer


def build_scheduler(optimizer, config: FinetuneConfig, steps_per_epoch: int):
    """æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    total_steps = config.num_epochs * steps_per_epoch
    warmup_steps = int(config.warmup_epochs * steps_per_epoch)  # ã€ä¿®å¤ã€‘æ”¯æŒå°æ•°warmup_epochs
    
    print(f"\nğŸ“Š æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨:")
    print(f"   æ€»æ­¥æ•°: {total_steps}")
    print(f"   Warmupæ­¥æ•°: {warmup_steps} ({config.warmup_epochs} epochs)")
    
    def lr_lambda(step):
        if step < warmup_steps:
            return (step + 1) / max(warmup_steps, 1)  # ã€ä¿®å¤ã€‘ä»step+1å¼€å§‹ï¼Œé¿å…lr=0
        else:
            progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)
            return 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)).item())
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    return scheduler


# ========================== æŸå¤±å‡½æ•° ==========================

class SegmentationLoss(nn.Module):
    """BCE + DiceæŸå¤± - ã€ä¿®å¤ã€‘æ”¯æŒAMPæ··åˆç²¾åº¦è®­ç»ƒ"""
    def __init__(self, bce_weight: float = 0.3, dice_weight: float = 0.7):
        super().__init__()
        self.bce_weight = bce_weight
        self.dice_weight = dice_weight
        
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        # ã€ä¿®å¤ã€‘åœ¨è®¡ç®—BCEå‰é€€å‡ºautocastï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        with torch.amp.autocast('cuda', enabled=False):
            # è½¬æ¢ä¸ºfloat32ä»¥ç¡®ä¿ç²¾åº¦
            pred = pred.float()
            target = target.float()
            
            # ç¡®ä¿é¢„æµ‹å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼Œé¿å…log(0)
            pred = torch.clamp(pred, min=1e-7, max=1-1e-7)
            
            # BCE Loss
            bce_loss = F.binary_cross_entropy(pred, target, reduction='mean')
            
            # Dice Loss
            smooth = 1e-5
            pred_flat = pred.view(-1)
            target_flat = target.view(-1)
            intersection = (pred_flat * target_flat).sum()
            dice_loss = 1 - (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)
            
            total_loss = self.bce_weight * bce_loss + self.dice_weight * dice_loss
        
        return total_loss


# ========================== æ¨¡å‹ä¿å­˜/åŠ è½½ ==========================

def save_finetuned_model(model, config: FinetuneConfig, epoch: int, optimizer=None, metrics=None):
    """ä¿å­˜å¾®è°ƒæ¨¡å‹"""
    os.makedirs(config.output_dir, exist_ok=True)
    save_path = os.path.join(config.output_dir, f"checkpoint_epoch_{epoch}.pth")
    
    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "config": config.to_dict(),
    }
    
    if optimizer is not None:
        checkpoint["optimizer_state_dict"] = optimizer.state_dict()
    if metrics is not None:
        checkpoint["metrics"] = metrics
        
    torch.save(checkpoint, save_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {save_path}")
    return save_path


def load_finetuned_model(model, checkpoint_path: str, strict: bool = True):
    """åŠ è½½å¾®è°ƒæ¨¡å‹"""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(checkpoint["model_state_dict"], strict=strict)
    
    print(f"âœ… å·²åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    print(f"   Epoch: {checkpoint.get('epoch', 'N/A')}")
    if "metrics" in checkpoint:
        print(f"   Metrics: {checkpoint['metrics']}")
    
    return model, checkpoint


# ========================== ç­–ç•¥æ¨è ==========================

def recommend_strategy(dataset_size: int, gpu_memory_gb: float, target_domain: str = "medical") -> str:
    print(f"\nğŸ“Š åˆ†æå¾®è°ƒæ¡ä»¶:")
    print(f"   æ•°æ®é›†å¤§å°: {dataset_size}")
    print(f"   GPUæ˜¾å­˜: {gpu_memory_gb} GB")
    
    if dataset_size < 100:
        strategy = "decoder_only"
    elif dataset_size < 500:
        strategy = "decoder_prompt"
    elif dataset_size < 2000:
        strategy = "encoder_partial"
    else:
        strategy = "full" if gpu_memory_gb >= 24 else "encoder_partial"
    
    print(f"   æ¨èç­–ç•¥: {strategy}")
    return strategy
