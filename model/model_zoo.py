# model_zoo.py
import os
import torch
import argparse
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from abc import ABC, abstractmethod
from typing import Optional, Union, Tuple, List
from segment_anything import sam_model_registry

# ---------------------------- åŸºç±» ----------------------------
class SegmentationModelBase(nn.Module, ABC):
    def __init__(self, device, cfg):
        super().__init__()
        self.device = device
        self.cfg = cfg
        self.name = cfg.get("name", "Unknown")
        self.prompt_type = cfg.get("prompt_type", "none")
        self.prompt_required = cfg.get("prompt_required", False)
        
    @abstractmethod
    def forward(self, img_tensor, input_boxes=None, target_mask=None):
        """
        Args:
            img_tensor: [B, C, H, W]
            input_boxes: æç¤ºä¿¡æ¯ (box/point/none)
            target_mask: [B, 1, H, W] ç”¨äºè®­ç»ƒæ—¶è®¡ç®—loss
        Returns:
            pred_masks: [B, 1, H, W]
            loss: Optional[Tensor] å¦‚æœæä¾›target_mask
        """
        pass

    def predict(self, img_tensor, input_boxes=None):
        """æ¨ç†æ¨¡å¼ï¼Œåªè¿”å›é¢„æµ‹ç»“æœ"""
        with torch.no_grad():
            return self.forward(img_tensor, input_boxes)


# ---------------------------- SAMç³»åˆ—æ¨¡å‹ ----------------------------
class MedSAMModel(SegmentationModelBase):
    def __init__(self, device, cfg):
        super().__init__(device, cfg)
        try:
            from transformers import SamModel, SamProcessor
            self.model = SamModel.from_pretrained(cfg["repo_id"]).to(device)
            self.processor = SamProcessor.from_pretrained(cfg["repo_id"])
            self.processor.image_processor.do_rescale = False
            print(f"âœ… æˆåŠŸåŠ è½½ {cfg['name']}")
        except Exception as e:
            print(f"âŒ åŠ è½½ {cfg['name']} å¤±è´¥: {e}")
            raise

    def forward(self, img_tensor, input_boxes=None, target_mask=None):
        img_tensor = img_tensor.to(self.device)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢¯åº¦
        if img_tensor.requires_grad:
            # å¯¹æŠ—æ”»å‡»æ¨¡å¼ï¼Œç¡®ä¿æ¢¯åº¦ä¿æŒ
            inputs = self.processor(
                images=img_tensor,
                input_boxes=[[input_boxes]] if input_boxes is not None else None,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            # å¦‚æœprocessoræ–­å¼€äº†æ¢¯åº¦ï¼Œå°è¯•ä¿®å¤
            if not inputs['pixel_values'].requires_grad:
                processed_shape = inputs['pixel_values'].shape
                other_inputs = {k: v for k, v in inputs.items() if k != 'pixel_values'}
                
                # æ‰‹åŠ¨å¤„ç†å›¾åƒä¿æŒæ¢¯åº¦
                if processed_shape[-2:] != img_tensor.shape[-2:]:
                    processed_img = F.interpolate(
                        img_tensor,
                        size=processed_shape[-2:],
                        mode="bilinear",
                        align_corners=False
                    )
                else:
                    processed_img = img_tensor
                
                inputs = {**other_inputs, 'pixel_values': processed_img}
        else:
            # æ¨ç†æ¨¡å¼ï¼Œæ­£å¸¸ä½¿ç”¨processor
            inputs = self.processor(
                images=img_tensor,
                input_boxes=[[input_boxes]] if input_boxes is not None else None,
                return_tensors="pt",
                padding=True
            ).to(self.device)
        
        # æ¨¡å‹å‰å‘ä¼ æ’­
        outputs = self.model(**inputs, multimask_output=False)
        pred_masks = outputs.pred_masks.sigmoid()
        
        # å¤„ç†ç»´åº¦
        if len(pred_masks.shape) == 5:
            pred_masks = pred_masks.squeeze(2)
        
        # è°ƒæ•´å°ºå¯¸åˆ°åŸå§‹å›¾åƒå¤§å°
        pred_masks = F.interpolate(
            pred_masks,
            size=img_tensor.shape[-2:],
            mode="bilinear",
            align_corners=False
        )
        
        # è®¡ç®—æŸå¤±
        if target_mask is not None:
            target_mask = target_mask.to(self.device)
            if target_mask.shape != pred_masks.shape:
                target_mask = F.interpolate(
                    target_mask,
                    size=pred_masks.shape[-2:],
                    mode="bilinear",
                    align_corners=False
                )
            with torch.amp.autocast('cuda', enabled=False):
            	loss = F.binary_cross_entropy(pred_masks.float(), target_mask.float())
            # loss = F.binary_cross_entropy(pred_masks, target_mask)
            return pred_masks, loss
        
        return pred_masks


class SAMModel(MedSAMModel):
    """ä¸ MedSAM å®Œå…¨ä¸€è‡´ï¼Œä»…æ¢æƒé‡"""
    pass


class SAMPointModel(SegmentationModelBase):
    """SAM ä½¿ç”¨å•ç‚¹æç¤º"""
    def __init__(self, device, cfg):
        super().__init__(device, cfg)
        try:
            from transformers import SamModel, SamProcessor
            self.model = SamModel.from_pretrained(cfg["repo_id"]).to(device)
            self.processor = SamProcessor.from_pretrained(cfg["repo_id"])
            self.processor.image_processor.do_rescale = False
            print(f"âœ… æˆåŠŸåŠ è½½ {cfg['name']}")
        except Exception as e:
            print(f"âŒ åŠ è½½ {cfg['name']} å¤±è´¥: {e}")
            raise

    def forward(self, img_tensor, input_points=None, target_mask=None):
        img_tensor = img_tensor.to(self.device)
        
        inputs = self.processor(
            images=img_tensor,
            input_points=[[input_points]] if input_points is not None else None,
            return_tensors="pt",
            padding=True
        ).to(self.device)
        
        outputs = self.model(**inputs, multimask_output=False)
        pred_masks = outputs.pred_masks.sigmoid()
        
        if len(pred_masks.shape) == 5:
            pred_masks = pred_masks.squeeze(2)
            
        pred_masks = F.interpolate(
            pred_masks,
            size=img_tensor.shape[-2:],
            mode="bilinear",
            align_corners=False
        )
        
        if target_mask is not None:
            target_mask = target_mask.to(self.device)
            # âœ… ä½¿ç”¨autocastå®‰å…¨çš„æ–¹å¼è®¡ç®—loss
            with torch.amp.autocast('cuda', enabled=False):
                loss = F.binary_cross_entropy(pred_masks.float(), target_mask.float())
            return pred_masks, loss
            
        return pred_masks


# ---------------------------- ä¼ ç»Ÿåˆ†å‰²æ¨¡å‹ ----------------------------
class UNetModel(SegmentationModelBase):
    def __init__(self, device, cfg):
        super().__init__(device, cfg)
        try:
            # ä½¿ç”¨ç®€å•çš„UNetå®ç°æˆ–è€…é¢„è®­ç»ƒæ¨¡å‹
            import torchvision.models.segmentation as seg_models
            self.model = seg_models.fcn_resnet50(pretrained=True, num_classes=1).to(device)
            
            # å¦‚æœæœ‰æœ¬åœ°æƒé‡ï¼ŒåŠ è½½
            if cfg.get("local_path") and os.path.exists(cfg["local_path"]):
                self.model.load_state_dict(torch.load(cfg["local_path"], map_location=device))
                print(f"âœ… ä» {cfg['local_path']} åŠ è½½æƒé‡")
            else:
                print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒæƒé‡åŠ è½½ {cfg['name']}")
        except Exception as e:
            print(f"âŒ åŠ è½½ {cfg['name']} å¤±è´¥: {e}")
            raise

    def forward(self, img_tensor, input_boxes=None, target_mask=None):
        img_tensor = img_tensor.to(self.device)
        
        # UNetä¸éœ€è¦æç¤ºï¼Œå¿½ç•¥input_boxes
        out = self.model(img_tensor)['out']
        pred_masks = torch.sigmoid(out)
        
        if target_mask is not None:
            target_mask = target_mask.to(self.device)
            if target_mask.shape != pred_masks.shape:
                target_mask = F.interpolate(
                    target_mask,
                    size=pred_masks.shape[-2:],
                    mode="bilinear",
                    align_corners=False
                )
            # âœ… ä½¿ç”¨autocastå®‰å…¨çš„æ–¹å¼è®¡ç®—loss
            with torch.amp.autocast('cuda', enabled=False):
                loss = F.binary_cross_entropy(pred_masks.float(), target_mask.float())
            return pred_masks, loss
            
        return pred_masks


class DeepLabV3Model(SegmentationModelBase):
    def __init__(self, device, cfg):
        super().__init__(device, cfg)
        try:
            import torchvision.models.segmentation as seg_models
            self.model = seg_models.deeplabv3_resnet101(pretrained=True, num_classes=1).to(device)
            
            if cfg.get("local_path") and os.path.exists(cfg["local_path"]):
                self.model.load_state_dict(torch.load(cfg["local_path"], map_location=device))
                print(f"âœ… ä» {cfg['local_path']} åŠ è½½æƒé‡")
            else:
                print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒæƒé‡åŠ è½½ {cfg['name']}")
        except Exception as e:
            print(f"âŒ åŠ è½½ {cfg['name']} å¤±è´¥: {e}")
            raise

    def forward(self, img_tensor, input_boxes=None, target_mask=None):
        img_tensor = img_tensor.to(self.device)
        
        out = self.model(img_tensor)['out']
        pred_masks = torch.sigmoid(out)
        
        if target_mask is not None:
            target_mask = target_mask.to(self.device)
            if target_mask.shape != pred_masks.shape:
                target_mask = F.interpolate(
                    target_mask,
                    size=pred_masks.shape[-2:],
                    mode="bilinear",
                    align_corners=False
                )
            loss = F.binary_cross_entropy(pred_masks, target_mask)
            return pred_masks, loss
            
        return pred_masks

# ---------------------------- SAM-Med2D ----------------------------

def create_official_args(model_type, ckpt_path, use_adapter, device):
    """å®Œå…¨å¤åˆ¶å®˜æ–¹çš„å‚æ•°åˆ›å»ºæ–¹å¼"""
    import argparse
    
    # å®Œå…¨å¤åˆ¶å®˜æ–¹çš„ parse_args å‡½æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--work_dir", type=str, default="workdir", help="work dir")
    parser.add_argument("--run_name", type=str, default="sammed", help="run model name")
    parser.add_argument("--batch_size", type=int, default=1, help="batch size")
    parser.add_argument("--image_size", type=int, default=256, help="image_size")
    parser.add_argument('--device', type=str, default=device)
    parser.add_argument("--data_path", type=str, default="data_demo", help="train data path") 
    parser.add_argument("--metrics", nargs='+', default=['iou', 'dice'], help="metrics")
    parser.add_argument("--model_type", type=str, default=model_type, help="sam model_type")
    parser.add_argument("--sam_checkpoint", type=str, default=ckpt_path, help="sam checkpoint")
    parser.add_argument("--boxes_prompt", type=bool, default=True, help="use boxes prompt")
    parser.add_argument("--point_num", type=int, default=1, help="point num")
    parser.add_argument("--iter_point", type=int, default=1, help="iter num") 
    parser.add_argument("--multimask", type=bool, default=True, help="ouput multimask")
    parser.add_argument("--encoder_adapter", type=bool, default=use_adapter, help="use adapter")
    parser.add_argument("--prompt_path", type=str, default=None, help="fix prompt path")
    parser.add_argument("--save_pred", type=bool, default=False, help="save reslut")
    
    # è§£æç©ºçš„å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨æ‰€æœ‰é»˜è®¤å€¼
    args = parser.parse_args([])
    
    # åº”ç”¨å®˜æ–¹çš„åå¤„ç†é€»è¾‘
    if args.iter_point > 1:
        args.iter_point = 1
        
    return args
  
# class SAMMed2DModel(SegmentationModelBase):
#     def __init__(self, device, cfg):
#         super().__init__(device, cfg)
#         from segment_anything import sam_model_registry
#         import os
    
#         model_type   = cfg.get("model_type", "vit_b")
#         ckpt_path    = cfg.get("local_path", "./pretrain_model/sam-med2d_b_clean.pth")
#         use_adapter  = cfg.get("encoder_adapter", True)
    
#         if not os.path.exists(ckpt_path):
#             raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
    
#         try:
#             # ä½¿ç”¨ä¸å®˜æ–¹å®Œå…¨ç›¸åŒçš„å‚æ•°åˆ›å»ºæ–¹å¼
#             args = create_official_args(model_type, ckpt_path, use_adapter, device)
            
#             print(f"åˆ›å»ºçš„ args å‚æ•°:")
#             for key, value in vars(args).items():
#                 print(f"  {key}: {value}")
                
#             # ä½¿ç”¨ä¸å®˜æ–¹å®Œå…¨ç›¸åŒçš„æ¨¡å‹åˆ›å»ºæ–¹å¼
#             self.model = sam_model_registry[args.model_type](args).to(args.device)
#             self.model.eval()
#             print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {model_type} from {ckpt_path}")
            
#         except Exception as e:
#             print(f"âŒ åŠ è½½ SAM-Med2D å¤±è´¥: {e}")
#             print("è¯·æ£€æŸ¥:")
#             print("1. segment_anything åº“ç‰ˆæœ¬æ˜¯å¦ä¸å®˜æ–¹ä¸€è‡´")
#             print("2. æƒé‡æ–‡ä»¶æ˜¯å¦æ­£ç¡®")
#             print("3. æ˜¯å¦ä½¿ç”¨äº†ä¿®æ”¹è¿‡çš„ SAM-Med2D å®ç°")
#             raise e

#     def forward(self, img_tensor, input_boxes=None, target_mask=None):
#         """
#         Args:
#             img_tensor: [B, C, H, W] in [0, 1]
#             input_boxes: list of [x1, y1, x2, y2] (åŸå§‹åæ ‡)
#             target_mask: [B, 1, H, W] ç”¨äºè®¡ç®— loss
#         Returns:
#             pred_masks: [B, 1, H, W]
#             loss: å¦‚æœæä¾›äº† target_mask
#         """
        
#         from segment_anything.utils.transforms import ResizeLongestSide
#         transform = ResizeLongestSide(self.model.image_encoder.img_size)
    
#         # è½¬æ¢ä¸º numpy å›¾åƒ
#         img_np = (img_tensor[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
#         original_size = img_np.shape[:2]

#         # print("original_size:", original_size)          # åº”è¯¥æ˜¯ (256, 256)
    
#         # å®˜æ–¹é¢„å¤„ç†
        
#         input_image = transform.apply_image(img_np)
#         input_image_torch = torch.as_tensor(input_image, device=self.device).permute(2, 0, 1).unsqueeze(0).float() / 255.0

#         # print("input_image_torch.shape:", input_image_torch.shape)  # åº”è¯¥æ˜¯ [1,3,H',W']ï¼ŒH'ã€W' â‰¤ 1024
    
#         # å›¾åƒç¼–ç 
#         with torch.no_grad():
#             image_embedding = self.model.image_encoder(input_image_torch)
    		
#         if input_boxes is not None:
#             # æ·»åŠ  bbox åæ ‡å˜æ¢
#             box_np = np.array([input_boxes])
#             box_tf = transform.apply_boxes(box_np, original_size)
#             box = torch.tensor(box_tf, device=self.device).unsqueeze(0).float()
            
#             print(f"åŸå§‹ bbox: {input_boxes}")
#             print(f"å˜æ¢å bbox: {box_tf[0]}")
#         else:
#             box = None

#         sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
#             points=None,
#             boxes=box,
#             masks=None,
#         )
    
#         # Mask è§£ç 
#         low_res_masks, iou_predictions = self.model.mask_decoder(
#             image_embeddings=image_embedding,
#             image_pe=self.model.prompt_encoder.get_dense_pe(),
#             sparse_prompt_embeddings=sparse_embeddings,
#             dense_prompt_embeddings=dense_embeddings,
#             multimask_output=False,
#         )
    
#         # åå¤„ç†ï¼šè¿˜åŸåˆ°åŸå›¾å°ºå¯¸
#         pred_masks = torch.sigmoid(low_res_masks)
#         pred_masks = F.interpolate(
#             pred_masks,
#             size=original_size,
#             mode="bilinear",
#             align_corners=False
#         )
    
#         # å¦‚æœæä¾›äº† target_maskï¼Œè®¡ç®— loss
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
#             if target_mask.shape != pred_masks.shape:
#                 target_mask = F.interpolate(target_mask, size=pred_masks.shape[-2:], mode="bilinear", align_corners=False)
#             loss = F.binary_cross_entropy(pred_masks, target_mask)
#             return pred_masks, loss
    
#         return pred_masks

# class SAMMed2DModel(SegmentationModelBase):
#     def __init__(self, device, cfg):
#         super().__init__(device, cfg)
#         from segment_anything import sam_model_registry
#         import os
    
#         model_type   = cfg.get("model_type", "vit_b")
#         ckpt_path    = cfg.get("local_path", "./pretrain_model/sam-med2d_b_clean.pth")
#         use_adapter  = cfg.get("encoder_adapter", True)
    
#         if not os.path.exists(ckpt_path):
#             raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
    
#         try:
#             # ğŸ”‘ ç›´æ¥ä½¿ç”¨é…ç½®ä¸­çš„image_sizeåˆ›å»ºargs
#             args = create_official_args(model_type, ckpt_path, use_adapter, device)
#             # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„image_size (ç°åœ¨æ˜¯256)
#             args.image_size = cfg.get("image_size", 256)
            
#             print(f"âœ… SAM-Med2D ä½¿ç”¨å›¾åƒå°ºå¯¸: {args.image_size}")
                
#             self.model = sam_model_registry[args.model_type](args).to(args.device)
#             self.model.eval()
#             print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {model_type} from {ckpt_path}")
            
#         except Exception as e:
#             print(f"âŒ åŠ è½½ SAM-Med2D å¤±è´¥: {e}")
#             raise e

#     def forward(self, img_tensor, input_boxes=None, target_mask=None):
#         """
#         Args:
#             img_tensor: [B, C, H, W] in [0, 1]
#             input_boxes: list of [x1, y1, x2, y2] (åŸå§‹åæ ‡)
#             target_mask: [B, 1, H, W] ç”¨äºè®¡ç®— loss
#         Returns:
#             pred_masks: [B, 1, H, W]
#             loss: å¦‚æœæä¾›äº† target_mask
#         """
        
#         from segment_anything.utils.transforms import ResizeLongestSide
        
#         # ä½¿ç”¨æ¨¡å‹çš„image_encoder.img_size (ç°åœ¨åº”è¯¥æ˜¯256)
#         transform = ResizeLongestSide(self.model.image_encoder.img_size)
        
#         # è½¬æ¢ä¸º numpy å›¾åƒ
#         img_np = (img_tensor[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
#         original_size = img_np.shape[:2]

#         print(f"ğŸ“ åŸå§‹å›¾åƒå°ºå¯¸: {original_size}")
#         print(f"ğŸ“ æ¨¡å‹ç›®æ ‡å°ºå¯¸: {self.model.image_encoder.img_size}")
        
#         # å®˜æ–¹é¢„å¤„ç†
#         input_image = transform.apply_image(img_np)
#         input_image_torch = torch.as_tensor(input_image, device=self.device).permute(2, 0, 1).unsqueeze(0).float() / 255.0

#         print(f"ğŸ“ å¤„ç†åå›¾åƒtensorå°ºå¯¸: {input_image_torch.shape}")

#         # å›¾åƒç¼–ç  
#         with torch.no_grad():
#             image_embedding = self.model.image_encoder(input_image_torch)

#         # ğŸ”‘ Prompt ç¼–ç ï¼šbbox åæ ‡å˜æ¢
#         if input_boxes is not None:
#             box_np = np.array([input_boxes])
#             box_tf = transform.apply_boxes(box_np, original_size)
#             box = torch.tensor(box_tf, device=self.device).unsqueeze(0).float()
            
#             print(f"ğŸ“¦ åŸå§‹ bbox: {input_boxes}")
#             print(f"ğŸ“¦ å˜æ¢å bbox: {box_tf[0]}")
#         else:
#             box = None

#         sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
#             points=None,
#             boxes=box,
#             masks=None,
#         )

#         # Mask è§£ç 
#         low_res_masks, iou_predictions = self.model.mask_decoder(
#             image_embeddings=image_embedding,
#             image_pe=self.model.prompt_encoder.get_dense_pe(),
#             sparse_prompt_embeddings=sparse_embeddings,
#             dense_prompt_embeddings=dense_embeddings,
#             multimask_output=False,
#         )

#         # åå¤„ç†ï¼šè¿˜åŸåˆ°åŸå›¾å°ºå¯¸
#         pred_masks = torch.sigmoid(low_res_masks)
#         pred_masks = F.interpolate(
#             pred_masks,
#             size=original_size,
#             mode="bilinear",
#             align_corners=False
#         )

#         # å¦‚æœæä¾›äº† target_maskï¼Œè®¡ç®— loss
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
#             if target_mask.shape != pred_masks.shape:
#                 target_mask = F.interpolate(target_mask, size=pred_masks.shape[-2:], mode="bilinear", align_corners=False)
#             loss = F.binary_cross_entropy(pred_masks, target_mask)
#             return pred_masks, loss

#         return pred_masks

# class SAMMed2DModel(SegmentationModelBase):
#     def __init__(self, device, cfg):
#         super().__init__(device, cfg)
#         from segment_anything import sam_model_registry
#         import os
    
#         model_type   = cfg.get("model_type", "vit_b")
#         ckpt_path    = cfg.get("local_path", "./pretrain_model/sam-med2d_b_clean.pth")
#         use_adapter  = cfg.get("encoder_adapter", True)
    
#         if not os.path.exists(ckpt_path):
#             raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
    
        
#         args = create_official_args(model_type, ckpt_path, use_adapter, device)
#         args.image_size = cfg.get("image_size", 256)
        
#         print(f"âœ… SAM-Med2D ä½¿ç”¨å›¾åƒå°ºå¯¸: {args.image_size}")
            
#         self.model = sam_model_registry[args.model_type](args).to(args.device)
#         # self.model = sam_model_registry[args.model_type](checkpoint=args.sam_checkpoint)
#         # self.model.eval()
#         print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {model_type} from {ckpt_path}")

#         print(f"âš ï¸ args.image_size = {args.image_size}")
#         print(f"âš ï¸ self.model.image_encoder.img_size = {self.model.image_encoder.img_size}")

#         # å¼ºåˆ¶æ‰€æœ‰æ¨¡å—è¿›å…¥è®­ç»ƒæ¨¡å¼
#         self.model.train()
#         self.model.image_encoder.train()
#         self.model.prompt_encoder.train()
#         self.model.mask_decoder.train()
        
#         # è§£å†» image_encoder çš„æ‰€æœ‰å‚æ•°
#         for param in self.model.image_encoder.parameters():
#             param.requires_grad = True

#     def forward(self, img_tensor, input_boxes=None, target_mask=None):
#         # åˆ é™¤æ‰€æœ‰numpyè½¬æ¢ä»£ç ï¼Œç›´æ¥ä½¿ç”¨PyTorchæ“ä½œ
#         from segment_anything.utils.transforms import ResizeLongestSide
#         print(f"ğŸ“ [SAMMed2D] è¾“å…¥å›¾åƒå°ºå¯¸: {img_tensor.shape}")

#         # ğŸ”‘ å…³é”®ä¿®å¤ï¼šç¡®ä¿è¾“å…¥å¼ é‡ç§»åˆ°æ­£ç¡®è®¾å¤‡
#         img_tensor = img_tensor.to(self.device)
        
#         # ğŸ”‘ é¢å¤–ä¿®å¤ï¼šç¡®ä¿target_maskä¹Ÿåœ¨æ­£ç¡®è®¾å¤‡ä¸Š  
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
            
#         print(f"ğŸ” [SAMMed2D] è¾“å…¥å›¾åƒå°ºå¯¸: {img_tensor.shape}")
#         print(f"ğŸ” [SAMMed2D] è¾“å…¥è®¾å¤‡: {img_tensor.device}")
#         print(f"ğŸ” [SAMMed2D] æ¨¡å‹è®¾å¤‡: {self.device}")
        
#         # ç›´æ¥ä½¿ç”¨åŸå§‹tensorï¼Œé¿å…numpyè½¬æ¢
#         original_size = img_tensor.shape[2:]  # [H, W]
        
#         # åˆ›å»ºçº¯PyTorchçš„é¢„å¤„ç†æµç¨‹
#         transform = ResizeLongestSide(self.model.image_encoder.img_size)
        
#         # PyTorchå®ç°çš„å›¾åƒç¼©æ”¾ (æ›¿æ¢apply_image)
#         scale = self.model.image_encoder.img_size / max(original_size)
#         new_h, new_w = int(original_size[0] * scale), int(original_size[1] * scale)
#         resized_tensor = F.interpolate(
#             img_tensor, 
#             size=(new_h, new_w), 
#             mode="bilinear", 
#             align_corners=False
#         )
        
#         # PyTorchå®ç°çš„padding (æ›¿æ¢apply_imageçš„padding)
#         h_pad = self.model.image_encoder.img_size - new_h
#         w_pad = self.model.image_encoder.img_size - new_w
#         input_image_torch = F.pad(
#             resized_tensor, 
#             (0, w_pad, 0, h_pad, 0, 0), 
#             value=0
#         )
        
#         print(f"ğŸ“ [SAMMed2D] å¤„ç†åå›¾åƒå°ºå¯¸: {input_image_torch.shape}")
        
#         # å›¾åƒç¼–ç  (ä¿æŒæ¢¯åº¦)
#         image_embedding = self.model.image_encoder(input_image_torch)
#         print(f"âœ… image_embedding.requires_grad: {image_embedding.requires_grad}")
        
#         # è¾¹ç•Œæ¡†å¤„ç† (çº¯PyTorch)
#         if input_boxes is not None:
#             # è¾¹ç•Œæ¡†åæ ‡ç¼©æ”¾
#             box_tensor = torch.tensor([input_boxes], device=self.device).float()
#             box_tensor[:, 0::2] *= scale  # xåæ ‡ç¼©æ”¾
#             box_tensor[:, 1::2] *= scale  # yåæ ‡ç¼©æ”¾
#             print(f"ğŸ“¦ [SAMMed2D] ç¼©æ”¾åbbox: {box_tensor[0].tolist()}")

      	
#         # ğŸ”‘ å…³é”®ä¿®å¤6ï¼šå®Œå…¨å¤åˆ¶testè„šæœ¬çš„promptç¼–ç 
#         sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
#             points=None,
#             boxes=box_tensor,
#             masks=None,
#         )

#         # ğŸ”‘ å…³é”®ä¿®å¤7ï¼šå®Œå…¨å¤åˆ¶testè„šæœ¬çš„maskè§£ç 
#         low_res_masks, iou_predictions = self.model.mask_decoder(
#             image_embeddings=image_embedding,
#             image_pe=self.model.prompt_encoder.get_dense_pe(),
#             sparse_prompt_embeddings=sparse_embeddings,
#             dense_prompt_embeddings=dense_embeddings,
#             multimask_output=False,
#         )

#         # ğŸ”‘ å…³é”®ä¿®å¤8ï¼šå®Œå…¨å¤åˆ¶testè„šæœ¬çš„åå¤„ç†
#         pred_masks = torch.sigmoid(low_res_masks)
        
#         # æ³¨æ„ï¼štestè„šæœ¬ä½¿ç”¨çš„æ˜¯image.shape[:2]ï¼Œè¿™é‡Œè¦ç”¨original_size
#         pred_masks = F.interpolate(
#             pred_masks,
#             size=original_size,
#             mode="bilinear",
#             align_corners=False
#         )
        
#         print(f"ğŸ“Š [SAMMed2D] æœ€ç»ˆpred_maskså½¢çŠ¶: {pred_masks.shape}")
#         print(f"ğŸ“Š [SAMMed2D] pred_masksèŒƒå›´: [{pred_masks.min().item():.3f}, {pred_masks.max().item():.3f}]")

#         # å¦‚æœæä¾›äº† target_maskï¼Œè®¡ç®— loss
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
#             if target_mask.shape != pred_masks.shape:
#                 target_mask = F.interpolate(target_mask, size=pred_masks.shape[-2:], mode="bilinear", align_corners=False)
#             loss = F.binary_cross_entropy(pred_masks, target_mask)
#             return pred_masks, loss

#         return pred_masks

#     # def forward(self, img_tensor, input_boxes=None, target_mask=None):
#     #     """
#     #     ä¿®å¤ç‰ˆï¼šä¸min_sammed2_test.pyå®Œå…¨ä¸€è‡´çš„å¤„ç†é€»è¾‘
#     #     Args:
#     #         img_tensor: [B, C, H, W] in [0, 1]
#     #         input_boxes: list of [x1, y1, x2, y2] (åŸå§‹åæ ‡)
#     #         target_mask: [B, 1, H, W] ç”¨äºè®¡ç®— loss
#     #     Returns:
#     #         pred_masks: [B, 1, H, W]
#     #         loss: å¦‚æœæä¾›äº† target_mask
#     #     """
        
#     #     from segment_anything.utils.transforms import ResizeLongestSide
        
#     #     # ğŸ”‘ å…³é”®ä¿®å¤1ï¼šç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„image_size
#     #     transform = ResizeLongestSide(self.model.image_encoder.img_size)
#     #     print(f"âš ï¸ ResizeLongestSideä½¿ç”¨çš„å°ºå¯¸: {self.model.image_encoder.img_size}")
#     #     # exit(0)
        
#     #     # ğŸ”‘ å…³é”®ä¿®å¤2ï¼šç²¾ç¡®å¤åˆ¶testè„šæœ¬çš„æ•°æ®è½¬æ¢
#     #     # é¿å…å¤šæ¬¡è½¬æ¢å¯¼è‡´çš„ç²¾åº¦æŸå¤±
#     #     if img_tensor.dim() == 4:  # [B, C, H, W]
#     #         img_np = (img_tensor[0].permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8)
#     #     else:  # [C, H, W]
#     #         img_np = (img_tensor.permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8)
        
#     #     original_size = img_np.shape[:2]
        
#     #     print(f"ğŸ“ [SAMMed2D] åŸå§‹å›¾åƒå°ºå¯¸: {original_size}")
#     #     print(f"ğŸ“ [SAMMed2D] æ¨¡å‹ç›®æ ‡å°ºå¯¸: {self.model.image_encoder.img_size}")
#     #     print(f"ğŸ“ [SAMMed2D] å›¾åƒæ•°æ®èŒƒå›´: [{img_np.min()}, {img_np.max()}]")
        
#     #     # ğŸ”‘ å…³é”®ä¿®å¤3ï¼šå®Œå…¨å¤åˆ¶testè„šæœ¬çš„é¢„å¤„ç†æµç¨‹
#     #     input_image = transform.apply_image(img_np)
#     #     input_image_torch = torch.as_tensor(input_image, device=self.device).permute(2, 0, 1).unsqueeze(0).float() / 255.0

#     #     print(f"ğŸ“ [SAMMed2D] å¤„ç†åå›¾åƒtensorå°ºå¯¸: {input_image_torch.shape}")
#     #     print(f"ğŸ“ [SAMMed2D] å¤„ç†åtensorèŒƒå›´: [{input_image_torch.min().item():.3f}, {input_image_torch.max().item():.3f}]")
				
#     #     # with torch.no_grad():
#     #     image_embedding = self.model.image_encoder(input_image_torch)
#     #     print("image_embedding.requires_grad:", image_embedding.requires_grad)
				
#     #     if input_boxes is not None:
#     #         # box_np = np.array([input_boxes])
#     #         # box_tf = transform.apply_boxes(box_np, original_size)
#     #         # box_torch = torch.tensor(box_tf, device=self.device).unsqueeze(0).float()
            
#     #         box_torch = torch.tensor([input_boxes], device=self.device).unsqueeze(0).float()
#     #         print(f"ğŸ“¦ [SAMMed2D] åŸå§‹ bbox: {input_boxes}")
#     #         # print(f"ğŸ“¦ [SAMMed2D] å˜æ¢å bbox: {box_tf[0]}")
#     #     else:
#     #         box_torch = None

#     #     # ğŸ”‘ å…³é”®ä¿®å¤6ï¼šå®Œå…¨å¤åˆ¶testè„šæœ¬çš„promptç¼–ç 
#     #     sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
#     #         points=None,
#     #         boxes=box_torch,
#     #         masks=None,
#     #     )

#     #     # ğŸ”‘ å…³é”®ä¿®å¤7ï¼šå®Œå…¨å¤åˆ¶testè„šæœ¬çš„maskè§£ç 
#     #     low_res_masks, iou_predictions = self.model.mask_decoder(
#     #         image_embeddings=image_embedding,
#     #         image_pe=self.model.prompt_encoder.get_dense_pe(),
#     #         sparse_prompt_embeddings=sparse_embeddings,
#     #         dense_prompt_embeddings=dense_embeddings,
#     #         multimask_output=False,
#     #     )

#     #     # ğŸ”‘ å…³é”®ä¿®å¤8ï¼šå®Œå…¨å¤åˆ¶testè„šæœ¬çš„åå¤„ç†
#     #     pred_masks = torch.sigmoid(low_res_masks)
        
#     #     # æ³¨æ„ï¼štestè„šæœ¬ä½¿ç”¨çš„æ˜¯image.shape[:2]ï¼Œè¿™é‡Œè¦ç”¨original_size
#     #     pred_masks = F.interpolate(
#     #         pred_masks,
#     #         size=original_size,
#     #         mode="bilinear",
#     #         align_corners=False
#     #     )
        
#     #     print(f"ğŸ“Š [SAMMed2D] æœ€ç»ˆpred_maskså½¢çŠ¶: {pred_masks.shape}")
#     #     print(f"ğŸ“Š [SAMMed2D] pred_masksèŒƒå›´: [{pred_masks.min().item():.3f}, {pred_masks.max().item():.3f}]")

#     #     # å¦‚æœæä¾›äº† target_maskï¼Œè®¡ç®— loss
#     #     if target_mask is not None:
#     #         target_mask = target_mask.to(self.device)
#     #         if target_mask.shape != pred_masks.shape:
#     #             target_mask = F.interpolate(target_mask, size=pred_masks.shape[-2:], mode="bilinear", align_corners=False)
#     #         loss = F.binary_cross_entropy(pred_masks, target_mask)
#     #         return pred_masks, loss

#     #     return pred_masks

#     def predict(self, img_tensor, input_boxes=None):
#         """æ¨ç†æ¨¡å¼ï¼Œåªè¿”å›é¢„æµ‹ç»“æœï¼Œä¿æŒä¸çˆ¶ç±»æ¥å£ä¸€è‡´"""
#         return self.forward(img_tensor, input_boxes)

# # ---------------------------- SAM-Med2D ----------------------------
# class SAMMed2DModel(SegmentationModelBase):
#     def __init__(self, device, cfg):
#         super().__init__(device, cfg)
#         from segment_anything import sam_model_registry
#         import os
        
#         # åŠ è½½é…ç½®
#         model_type = cfg.get("model_type", "vit_b")
#         ckpt_path = cfg.get("local_path", "./pretrain_model/sam-med2d_b.pth")
#         use_adapter = cfg.get("encoder_adapter", True)
        
#         if not os.path.exists(ckpt_path):
#             raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
        
#         # åˆ›å»ºå®˜æ–¹æ ¼å¼çš„argså¯¹è±¡
#         args = create_official_args(model_type, ckpt_path, use_adapter, device)
#         args.image_size = cfg.get("image_size", 256)
        
#         print(f"âœ… SAM-Med2D é…ç½®: æ¨¡å‹={model_type}, å°ºå¯¸={args.image_size}, é€‚é…å™¨={use_adapter}")
        
#         # åŠ è½½æ¨¡å‹ï¼ˆå…³é”®ä¿®å¤ï¼šä¿æŒæ¢¯åº¦ï¼‰
#         self.model = sam_model_registry[args.model_type](args).to(args.device)
#         self.model.train()  # âœ… å¿…é¡»è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥æ”¯æŒå¯¹æŠ—æ”»å‡»
#         print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {ckpt_path}")

#     def forward(self, img_tensor, input_boxes=None, target_mask=None):
#         """
#         å®Œå…¨å¤ç”¨ä¹‹å‰æˆåŠŸçš„å®ç°é€»è¾‘
#         """
#         from segment_anything.utils.transforms import ResizeLongestSide
        
#         # ç¡®ä¿è®¾å¤‡ä¸€è‡´
#         img_tensor = img_tensor.to(self.device)
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
        
#         # è·å–åŸå§‹å°ºå¯¸
#         original_size = img_tensor.shape[2:]  # (H, W)
        
#         # å®˜æ–¹é¢„å¤„ç†
#         transform = ResizeLongestSide(self.model.image_encoder.img_size)
#         scale = self.model.image_encoder.img_size / max(original_size)
#         new_h, new_w = int(original_size[0] * scale), int(original_size[1] * scale)
        
#         # ç¼©æ”¾
#         resized_tensor = F.interpolate(img_tensor, size=(new_h, new_w), 
#                                      mode="bilinear", align_corners=False)
        
#         # Padding
#         h_pad = self.model.image_encoder.img_size - new_h
#         w_pad = self.model.image_encoder.img_size - new_w
#         input_image_torch = F.pad(resized_tensor, (0, w_pad, 0, h_pad, 0, 0), value=0)
        
#         # å›¾åƒç¼–ç ï¼ˆä¿æŒæ¢¯åº¦ï¼‰
#         image_embedding = self.model.image_encoder(input_image_torch)
        
#         # è¾¹ç•Œæ¡†å¤„ç†
#         box_tensor = None
#         if input_boxes is not None:
#             box_tensor = torch.tensor([input_boxes], device=self.device).float()
#             box_tensor[:, 0::2] *= scale  # xåæ ‡ç¼©æ”¾
#             box_tensor[:, 1::2] *= scale  # yåæ ‡ç¼©æ”¾
        
#         # Promptç¼–ç 
#         sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
#             points=None, boxes=box_tensor, masks=None
#         )
        
#         # Maskè§£ç 
#         low_res_masks, iou_predictions = self.model.mask_decoder(
#             image_embeddings=image_embedding,
#             image_pe=self.model.prompt_encoder.get_dense_pe(),
#             sparse_prompt_embeddings=sparse_embeddings,
#             dense_prompt_embeddings=dense_embeddings,
#             multimask_output=False,
#         )
        
#         # åå¤„ç†
#         pred_masks = torch.sigmoid(low_res_masks)
#         pred_masks = F.interpolate(pred_masks, size=original_size, 
#                                  mode="bilinear", align_corners=False)
        
#         # è®¡ç®—æŸå¤±
#         if target_mask is not None:
#             if target_mask.shape != pred_masks.shape:
#                 target_mask = F.interpolate(target_mask, size=pred_masks.shape[-2:], 
#                                           mode="bilinear", align_corners=False)
#             loss = F.binary_cross_entropy(pred_masks, target_mask)
#             return pred_masks, loss
        
#         return pred_masks

#     def predict(self, img_tensor, input_boxes=None):
#         """æ¨ç†æ¨¡å¼"""
#         return self.forward(img_tensor, input_boxes)

# ---------------------------- SAM-Med2D ----------------------------
# class SAMMed2DModel(SegmentationModelBase):
#     def __init__(self, device, cfg):
#         super().__init__(device, cfg)
#         from segment_anything import sam_model_registry
#         import os
        
#         # åŠ è½½é…ç½®
#         model_type = cfg.get("model_type", "vit_b")
#         ckpt_path = cfg.get("local_path", "./pretrain_model/sam-med2d_b.pth")
#         use_adapter = cfg.get("encoder_adapter", True)
        
#         if not os.path.exists(ckpt_path):
#             raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
        
#         # åˆ›å»ºå®˜æ–¹æ ¼å¼çš„argså¯¹è±¡
#         args = create_official_args(model_type, ckpt_path, use_adapter, device)
#         args.image_size = cfg.get("image_size", 256)
        
#         print(f"âœ… SAM-Med2D é…ç½®: æ¨¡å‹={model_type}, å°ºå¯¸={args.image_size}, é€‚é…å™¨={use_adapter}")
        
#         # åŠ è½½æ¨¡å‹å¹¶ä¿æŒè®­ç»ƒæ¨¡å¼ï¼ˆç”¨äºå¯¹æŠ—æ”»å‡»ï¼‰
#         self.model = sam_model_registry[args.model_type](args).to(args.device)
#         self.model.train()
#         print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {ckpt_path}")

#     def forward(self, img_tensor, input_boxes=None, target_mask=None):
#         """
#         ä¿®å¤ç‰ˆï¼šç›´æ¥ä½¿ç”¨é¢„å¤„ç†åçš„è¾“å…¥ï¼Œé¿å…åŒé‡é¢„å¤„ç†
#         """
#         from segment_anything.utils.transforms import ResizeLongestSide
        
#         # ç¡®ä¿è®¾å¤‡ä¸€è‡´
#         img_tensor = img_tensor.to(self.device)
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
        
#         # è·å–å°ºå¯¸ï¼ˆåº”è¯¥æ˜¯256x256ï¼‰
#         _, _, H, W = img_tensor.shape
#         original_size = (H, W)
        
#         # âœ… å…³é”®ä¿®å¤ï¼šè·³è¿‡é‡å¤é¢„å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥ï¼ˆå€¼èŒƒå›´å·²æ˜¯[0,1]ï¼‰
#         input_image_torch = img_tensor
        
#         # å›¾åƒç¼–ç ï¼ˆä¿æŒæ¢¯åº¦ç”¨äºå¯¹æŠ—æ”»å‡»ï¼‰   ï¼‰
#         image_embedding = self.model.image_encoder(input_image_torch)
        
#         # âœ… ä¿®å¤ï¼šæ­£ç¡®å¤„ç†bboxæ ¼å¼ [1, 1, 4]
#         box_tensor = None
#         if input_boxes is not None:
#             if isinstance(input_boxes, (list, tuple, np.ndarray)):
#                 box_tensor = torch.tensor([[input_boxes]], device=self.device).float()  # [1, 1, 4]
#             else:
#                 box_tensor = input_boxes.float().to(self.device)
#                 if box_tensor.dim() == 2:
#                     box_tensor = box_tensor.unsqueeze(0)  # [1, 1, 4]
        
#         # Promptç¼–ç 
#         sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
#             points=None, boxes=box_tensor, masks=None
#         )
        
#         # Maskè§£ç 
#         low_res_masks, iou_predictions = self.model.mask_decoder(
#             image_embeddings=image_embedding,
#             image_pe=self.model.prompt_encoder.get_dense_pe(),
#             sparse_prompt_embeddings=sparse_embeddings,
#             dense_prompt_embeddings=dense_embeddings,
#             multimask_output=False,
#         )
        
#         # åå¤„ç†
#         pred_masks = torch.sigmoid(low_res_masks)
#         pred_masks = F.interpolate(pred_masks, size=original_size, 
#                                  mode="bilinear", align_corners=False)
        
#         # è®¡ç®—æŸå¤±
#         if target_mask is not None:
#             if target_mask.shape != pred_masks.shape:
#                 target_mask = F.interpolate(target_mask, size=pred_masks.shape[-2:], 
#                                           mode="bilinear", align_corners=False)
#             loss = F.binary_cross_entropy(pred_masks, target_mask)
#             return pred_masks, loss
        
#         return pred_masks

#     def predict(self, img_tensor, input_boxes=None):
#         """æ¨ç†æ¨¡å¼"""
#         return self.forward(img_tensor, input_boxes)

# class SAMMed2DModel(SegmentationModelBase):
#     def __init__(self, device, cfg):
#         super().__init__(device, cfg)
#         from segment_anything import sam_model_registry
        
#         # åŠ è½½é…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
#         model_type = cfg.get("model_type", "vit_b")
#         ckpt_path = cfg.get("local_path", "./pretrain_model/sam-med2d_b.pth")
#         use_adapter = cfg.get("encoder_adapter", True)
        
#         if not os.path.exists(ckpt_path):
#             raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
        
#         # âœ… åŠ è½½æ¨¡å‹ï¼ˆå¿…é¡»trainæ¨¡å¼æ”¯æŒå¯¹æŠ—æ”»å‡»æ¢¯åº¦ï¼‰
#         args = create_official_args(model_type, ckpt_path, use_adapter, device)
#         args.image_size = cfg.get("image_size", 256)
#         self.model = sam_model_registry[args.model_type](args).to(args.device)
#         self.model.train()  # âœ… å…³é”®ï¼šè®­ç»ƒæ¨¡å¼
#         self.image_size = args.image_size
        
#         print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {model_type}, å›¾åƒå°ºå¯¸: {self.image_size}")
        
#         # âœ… åˆå§‹åŒ–å®˜æ–¹é¢„å¤„ç†ï¼ˆç”¨äºå†…éƒ¨åæ ‡å˜æ¢ï¼‰
#         from segment_anything.utils.transforms import ResizeLongestSide
#         self.transform = ResizeLongestSide(self.image_size)

#     def forward(self, img_tensor, input_boxes=None, target_mask=None):
#         """
#         âœ… å®Œå…¨å¤ç°SammedPredictoré€»è¾‘ï¼Œä½†çº¯PyTorchå®ç°ä¿æŒæ¢¯åº¦
#         """
#         # è®¾å¤‡åŒæ­¥
#         img_tensor = img_tensor.to(self.device)
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
        
#         # âœ… è·å–åŸå§‹å°ºå¯¸ï¼ˆåº”ä¸º256x256ï¼‰
#         batch_size, _, H, W = img_tensor.shape
#         original_size = (H, W)
        
#         # âœ… å°†[0,1]tensorè½¬æ¢ä¸ºuint8 numpyï¼ˆä¸ç‹¬ç«‹å®éªŒä¸€è‡´ï¼‰
#         # æ³¨æ„ï¼šè¿™æ­¥ä¼šä¸¢å¤±æ¢¯åº¦ï¼Œä½†SammedPredictorå†…éƒ¨å°±æ˜¯è¿™ä¹ˆåšçš„
#         # å…³é”®åœ¨äºåç»­æ‰€æœ‰æ“ä½œéƒ½æ˜¯PyTorch
#         img_np = (img_tensor[0].permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8)
        
#         # âœ… å¤ç°set_image()çš„é¢„å¤„ç†
#         input_image = self.transform.apply_image(img_np)
#         input_image_torch = torch.as_tensor(input_image, device=self.device).permute(2, 0, 1).unsqueeze(0).float() / 255.0
        
#         # âœ… å›¾åƒç¼–ç ï¼ˆä¿æŒæ¢¯åº¦ï¼‰
#         image_embedding = self.model.image_encoder(input_image_torch)
        
#         # âœ… å¤ç°predict()çš„bboxå¤„ç†ï¼ˆæ ¼å¼ï¼š[1, 1, 4]ï¼‰
#         box_tensor = None
#         if input_boxes is not None:
#             # åæ ‡ç¼©æ”¾ï¼ˆå› ä¸ºinput_image_torchå°ºå¯¸å¯èƒ½>256ï¼‰
#             scale = self.image_size / max(original_size)
            
#             # ç»Ÿä¸€è½¬æ¢ä¸ºnumpy
#             if isinstance(input_boxes, (list, tuple)):
#                 box_np = np.array(input_boxes)
#             elif isinstance(input_boxes, np.ndarray):
#                 box_np = input_boxes
#             else:  # tensor
#                 box_np = input_boxes.detach().cpu().numpy()
            
#             # åº”ç”¨å®˜æ–¹å˜æ¢ï¼ˆç¼©æ”¾+paddingï¼‰
#             box_tf = self.transform.apply_boxes(box_np.reshape(1, 4), original_size)
#             box_tensor = torch.tensor(box_tf, device=self.device).unsqueeze(0).float()
        
#         # âœ… Promptç¼–ç 
#         sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
#             points=None, boxes=box_tensor, masks=None
#         )
        
#         # âœ… Maskè§£ç 
#         low_res_masks, iou_predictions = self.model.mask_decoder(
#             image_embeddings=image_embedding,
#             image_pe=self.model.prompt_encoder.get_dense_pe(),
#             sparse_prompt_embeddings=sparse_embeddings,
#             dense_prompt_embeddings=dense_embeddings,
#             multimask_output=False,
#         )
        
#         # âœ… åå¤„ç†ï¼ˆsigmoid + æ’å€¼å›åŸå°ºå¯¸ï¼‰
#         pred_masks = torch.sigmoid(low_res_masks)
#         pred_masks = F.interpolate(pred_masks, size=original_size, 
#                                  mode="bilinear", align_corners=False)
        
#         # âœ… è®¡ç®—å¯¹æŠ—æ”»å‡»æ‰€éœ€loss
#         if target_mask is not None:
#             if target_mask.shape != pred_masks.shape:
#                 target_mask = F.interpolate(target_mask, size=pred_masks.shape[-2:], 
#                                           mode="bilinear", align_corners=False)
#             loss = F.binary_cross_entropy(pred_masks, target_mask)
#             return pred_masks, loss
        
#         return pred_masks
# model_zoo.py - åªä¿®æ”¹ SAMMed2DModel ç±»
# model_zoo.py - ä¿®å¤ SAM-Med2D éƒ¨åˆ†
# class SAMMed2DModel(SegmentationModelBase):
#     def __init__(self, device, cfg):
#         super().__init__(device, cfg)
#         from segment_anything import sam_model_registry
#         from segment_anything.predictor_sammed import SammedPredictor
        
#         # åŠ è½½é…ç½®
#         model_type = cfg.get("model_type", "vit_b")
#         ckpt_path = cfg.get("local_path", "./pretrain_model/sam-med2d_b.pth")
#         image_size = cfg.get("image_size", 256)
#         use_adapter = cfg.get("encoder_adapter", True)
        
#         if not os.path.exists(ckpt_path):
#             raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
        
#         # âœ… 1. åˆ›å»ºä¸ç‹¬ç«‹å®ç°å®Œå…¨ä¸€è‡´çš„ args
#         args = argparse.Namespace(
#             image_size=image_size,
#             encoder_adapter=use_adapter,
#             sam_checkpoint=ckpt_path,
#             model_type=model_type,
#             device=device
#         )
        
#         # âœ… 2. ä½¿ç”¨ sam_model_registry åŠ è½½æ¨¡å‹ï¼ˆä¸ç‹¬ç«‹å®ç°ä¸€è‡´ï¼‰
#         self.model = sam_model_registry[model_type](args).to(device)
#         self.model.eval()  # âœ… å…³é”®ï¼šè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
#         # âœ… 3. åˆ›å»ºå®˜æ–¹ predictorï¼ˆä¸ç‹¬ç«‹å®ç°å®Œå…¨ä¸€è‡´ï¼‰
#         self.predictor = SammedPredictor(self.model)
#         self.image_size = image_size
        
#         print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {model_type}, å›¾åƒå°ºå¯¸: {image_size}")
#         print(f"âœ… ä½¿ç”¨å®˜æ–¹ SammedPredictorï¼Œä¸ç‹¬ç«‹æµ‹è¯•å®ç°å®Œå…¨ä¸€è‡´")

#     def forward(self, img_tensor, input_boxes=None, target_mask=None):
#         """
#         Args:
#             img_tensor: [B, C, H, W] in [0, 1]
#             input_boxes: list of [x1, y1, x2, y2] (åŸå§‹åæ ‡)
#             target_mask: [B, 1, H, W] ç”¨äºè®¡ç®— loss
#         Returns:
#             pred_masks: [B, 1, H, W]
#             loss: å¦‚æœæä¾›äº† target_mask
#         """
#         # âœ… è®¾å¤‡åŒæ­¥ï¼ˆå¯¹æŠ—æ”»å‡»æ—¶éœ€è¦ï¼‰
#         img_tensor = img_tensor.to(self.device)
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
        
#         # è·å–æ‰¹æ¬¡å’Œå°ºå¯¸
#         batch_size, _, H, W = img_tensor.shape
#         original_size = (H, W)
        
#         # âœ… 4. å¤ç° predictor.set_image() çš„å®Œæ•´é€»è¾‘
#         # å°† [0,1] tensor è½¬æ¢ä¸º uint8 numpyï¼ˆä¸ç‹¬ç«‹å®ç°ä¸€è‡´ï¼‰
#         img_np = (img_tensor[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
        
#         # âœ… 5. è°ƒç”¨ predictor.set_image()ï¼ˆè‡ªåŠ¨å®Œæˆæ‰€æœ‰é¢„å¤„ç†ï¼‰
#         self.predictor.set_image(img_np)
        
#         # âœ… 6. å‡†å¤‡ bbox æ ¼å¼ï¼ˆpredictor.predict éœ€è¦çš„æ ¼å¼ï¼‰
#         box_input = np.array([input_boxes]) if input_boxes is not None else None
        
#         # âœ… 7. è°ƒç”¨ predictor.predict()ï¼ˆä¸ç‹¬ç«‹å®ç°å®Œå…¨ä¸€è‡´ï¼‰
#         masks, iou_predictions, low_res_logits = self.predictor.predict(
#             point_coords=None,
#             point_labels=None,
#             box=box_input,
#             multimask_output=False
#         )
        
#         # masks æ˜¯ numpy arrayï¼Œéœ€è¦è½¬æ¢ä¸º tensor
#         pred_masks = torch.from_numpy(masks).float().to(self.device)
#         pred_masks = pred_masks.unsqueeze(0)  # [1, 1, H, W]
        
#         # âœ… 8. è®¡ç®—å¯¹æŠ—æ”»å‡»æ‰€éœ€çš„ loss
#         if target_mask is not None:
#             # ç¡®ä¿å°ºå¯¸åŒ¹é…
#             if pred_masks.shape != target_mask.shape:
#                 target_mask = F.interpolate(target_mask, size=pred_masks.shape[-2:], 
#                                           mode="bilinear", align_corners=False)
#             loss = F.binary_cross_entropy(pred_masks, target_mask)
#             return pred_masks, loss
        
#         return pred_masks
    
#     def predict(self, img_tensor, input_boxes=None):
#         """æ¨ç†æ¨¡å¼"""
#         return self.forward(img_tensor, input_boxes)
# model_zoo.py - åªæ›¿æ¢ SAMMed2DModel ç±»

# class SAMMed2DModel(SegmentationModelBase):
#     def __init__(self, device, cfg):
#         super().__init__(device, cfg)
#         from segment_anything import sam_model_registry
#         from segment_anything.utils.transforms import ResizeLongestSide
        
#         # åŠ è½½é…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
#         model_type = cfg.get("model_type", "vit_b")
#         ckpt_path = cfg.get("local_path", "./pretrain_model/sam-med2d_b.pth")
#         image_size = cfg.get("image_size", 256)
#         use_adapter = cfg.get("encoder_adapter", True)
        
#         if not os.path.exists(ckpt_path):
#             raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
        
#         # âœ… åˆ›å»ºargsï¼ˆä¸ç‹¬ç«‹å®ç°ä¸€è‡´ï¼‰
#         args = argparse.Namespace(
#             image_size=image_size,
#             encoder_adapter=use_adapter,
#             sam_checkpoint=ckpt_path,
#             model_type=model_type,
#             device=device
#         )
        
#         # âœ… åŠ è½½æ¨¡å‹
#         self.model = sam_model_registry[model_type](args).to(device)
#         self.model.eval()  # é»˜è®¤è¯„ä¼°æ¨¡å¼
        
#         # âœ… å…³é”®ï¼šåˆå§‹åŒ–å®˜æ–¹å˜æ¢å™¨ï¼ˆç”¨äºbboxåæ ‡ç¼©æ”¾ï¼‰
#         self.transform = ResizeLongestSide(image_size)
#         self.image_size = image_size
        
#         print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {model_type}, å›¾åƒå°ºå¯¸: {image_size}")
#         print(f"âœ… ä½¿ç”¨çº¯PyTorchå®ç°ï¼Œä¿æŒæ¢¯åº¦æµç”¨äºå¯¹æŠ—æ”»å‡»")

#     def forward(self, img_tensor, input_boxes=None, target_mask=None):
#         """
#         âœ… çº¯PyTorchå®ç°ï¼Œé€è¡Œå¯¹ç…§ SammedPredictor çš„ set_image() å’Œ predict()
#         """
#         # âœ… è®¾å¤‡åŒæ­¥
#         img_tensor = img_tensor.to(self.device)
#         if target_mask is not None:
#             target_mask = target_mask.to(self.device)
        
#         # è·å–å°ºå¯¸
#         batch_size, _, H, W = img_tensor.shape
#         original_size = (H, W)
        
#         # ============================================================
#         # âœ… é˜¶æ®µ1ï¼šå›¾åƒé¢„å¤„ç†ï¼ˆå¯¹åº” SammedPredictor.set_image()ï¼‰
#         # ============================================================
#         # ç¼©æ”¾é€»è¾‘ï¼šResizeLongestSide.apply_image()
#         scale = self.image_size / max(original_size)
#         new_h, new_w = int(H * scale), int(W * scale)
#         resized_tensor = F.interpolate(
#             img_tensor, 
#             size=(new_h, new_w), 
#             mode="bilinear", 
#             align_corners=False  # ä¸å®˜æ–¹ä¸€è‡´
#         )
        
#         # Paddingé€»è¾‘ï¼šResizeLongestSide.apply_image()
#         h_pad = self.image_size - new_h
#         w_pad = self.image_size - new_w
#         input_image_torch = F.pad(resized_tensor, (0, w_pad, 0, h_pad, 0, 0), value=0)
        
#         # âœ… å›¾åƒç¼–ç ï¼ˆå¯¹åº” SammedPredictor.set_image() çš„ model.image_encoderï¼‰
#         image_embedding = self.model.image_encoder(input_image_torch)
        
#         # ============================================================
#         # âœ… é˜¶æ®µ2ï¼šPromptå¤„ç†ï¼ˆå¯¹åº” SammedPredictor.predict()ï¼‰
#         # ============================================================
#         # âœ… å…³é”®ï¼šbboxåæ ‡å˜æ¢å¿…é¡»ä½¿ç”¨å®˜æ–¹ transform.apply_boxes()
#         # è¿™æ˜¯IOUç²¾åº¦çš„æ ¸å¿ƒä¿è¯ï¼
#         box_tensor = None
#         if input_boxes is not None:
#             # è½¬æ¢ä¸ºnumpyï¼ˆå› ä¸ºapply_boxesæœŸæœ›numpyè¾“å…¥ï¼‰
#             if isinstance(input_boxes, (list, tuple, np.ndarray)):
#                 box_np = np.array(input_boxes).reshape(1, 4)
#             else:  # tensor
#                 box_np = input_boxes.detach().cpu().numpy().reshape(1, 4)
            
#             # âœ… ä½¿ç”¨å®˜æ–¹å˜æ¢ï¼ˆç²¾ç¡®å¤ç°ï¼‰
#             box_tf = self.transform.apply_boxes(box_np, original_size)
#             box_tensor = torch.tensor(box_tf, device=self.device).unsqueeze(0).float()
        
#         # âœ… Promptç¼–ç ï¼ˆä¸å®˜æ–¹å®Œå…¨ä¸€è‡´ï¼‰
#         sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
#             points=None, 
#             boxes=box_tensor, 
#             masks=None
#         )
        
#         # ============================================================
#         # âœ… é˜¶æ®µ3ï¼šMaskè§£ç ä¸åå¤„ç†ï¼ˆå¯¹åº” SammedPredictor.predict()ï¼‰
#         # ============================================================
#         low_res_masks, iou_predictions = self.model.mask_decoder(
#             image_embeddings=image_embedding,
#             image_pe=self.model.prompt_encoder.get_dense_pe(),
#             sparse_prompt_embeddings=sparse_embeddings,
#             dense_prompt_embeddings=dense_embeddings,
#             multimask_output=False,
#         )
        
#         # âœ… åå¤„ç†ï¼ˆsigmoid + æ’å€¼åˆ°åŸå›¾å°ºå¯¸ï¼‰
#         pred_masks = torch.sigmoid(low_res_masks)
#         pred_masks = F.interpolate(
#             pred_masks, 
#             size=original_size, 
#             mode="bilinear", 
#             align_corners=False
#         )
        
#         # ============================================================
#         # âœ… é˜¶æ®µ4ï¼šå¯¹æŠ—æ”»å‡»æŸå¤±è®¡ç®—
#         # ============================================================
#         if target_mask is not None:
#             if target_mask.shape != pred_masks.shape:
#                 target_mask = F.interpolate(
#                     target_mask, 
#                     size=pred_masks.shape[-2:], 
#                     mode="bilinear", 
#                     align_corners=False
#                 )
#             loss = F.binary_cross_entropy(pred_masks, target_mask)
#             return pred_masks, loss
        
#         return pred_masks
    
#     def predict(self, img_tensor, input_boxes=None):
#         """æ¨ç†æ¨¡å¼ï¼ˆä¿æŒæ¥å£ä¸€è‡´ï¼‰"""
#         return self.forward(img_tensor, input_boxes)
# model_zoo.py - ç›´æ¥æ›¿æ¢æ•´ä¸ª SAMMed2DModel ç±»


class SAMMed2DModel(SegmentationModelBase):
    def __init__(self, device, cfg):
        super().__init__(device, cfg)
        from segment_anything import sam_model_registry
        from segment_anything.utils.transforms import ResizeLongestSide
        
        # åŠ è½½é…ç½®
        model_type = cfg.get("model_type", "vit_b")
        ckpt_path = cfg.get("local_path", "./pretrain_model/sam-med2d_b.pth")
        image_size = cfg.get("image_size", 256)
        use_adapter = cfg.get("encoder_adapter", True)
        
        if not os.path.exists(ckpt_path):
            raise FileNotFoundError(f"âŒ SAM-Med2D æƒé‡æœªæ‰¾åˆ°: {ckpt_path}")
        
        # åˆ›å»ºargsï¼ˆä¸ç‹¬ç«‹æµ‹è¯•è„šæœ¬ä¸€è‡´ï¼‰
        args = argparse.Namespace(
            image_size=image_size,
            encoder_adapter=use_adapter,
            sam_checkpoint=ckpt_path,
            model_type=model_type,
            device=device
        )
        
        # åŠ è½½æ¨¡å‹
        self.model = sam_model_registry[model_type](args).to(device)
        self.model.eval()  # è¯„ä¼°æ¨¡å¼ï¼Œä½†åœ¨å¯¹æŠ—æ”»å‡»æ—¶ä¼šä¸´æ—¶åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
        
        # åˆå§‹åŒ–å®˜æ–¹å˜æ¢å™¨ï¼ˆç”¨äºbboxåæ ‡ç¼©æ”¾ï¼‰
        self.transform = ResizeLongestSide(image_size)
        self.image_size = image_size
        
        # âœ… ä¿®å¤1ï¼šç¡®ä¿pixel_mean/stdåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šåˆå§‹åŒ–
        pixel_mean = torch.tensor([123.675, 116.28, 103.53], device=device).view(-1, 1, 1)
        pixel_std = torch.tensor([58.395, 57.12, 57.375], device=device).view(-1, 1, 1)
        self.register_buffer("pixel_mean", pixel_mean)
        self.register_buffer("pixel_std", pixel_std)
        
        print(f"âœ… æˆåŠŸåŠ è½½ SAM-Med2D: {model_type}, å›¾åƒå°ºå¯¸: {image_size}")
        print(f"âœ… å·²æ³¨å†Œæ­£ç¡®çš„å½’ä¸€åŒ–å‚æ•°ï¼Œä¸ç‹¬ç«‹æµ‹è¯•è„šæœ¬ä¿æŒä¸€è‡´")

    def forward(self, img_tensor, input_boxes=None, target_mask=None):
        """
        âœ… çº¯PyTorchå®ç°ï¼Œæ­£ç¡®å¤„ç†å½’ä¸€åŒ–ï¼Œä¿æŒæ¢¯åº¦æµç”¨äºå¯¹æŠ—æ”»å‡»
        """
        # âœ… ä¿®å¤2ï¼šç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        img_tensor = img_tensor.to(self.device)
        if target_mask is not None:
            target_mask = target_mask.to(self.device)
        
        # è·å–å°ºå¯¸
        batch_size, _, H, W = img_tensor.shape
        original_size = (H, W)
        
        # ============================================================
        # âœ… é˜¶æ®µ1ï¼šå›¾åƒé¢„å¤„ç†
        # ============================================================
        # ç¼©æ”¾é€»è¾‘
        scale = self.image_size / max(original_size)
        new_h, new_w = int(H * scale), int(W * scale)
        resized_tensor = F.interpolate(
            img_tensor, 
            size=(new_h, new_w), 
            mode="bilinear", 
            align_corners=False
        )
        
        # Paddingé€»è¾‘
        h_pad = self.image_size - new_h
        w_pad = self.image_size - new_w
        input_image_torch = F.pad(resized_tensor, (0, w_pad, 0, h_pad, 0, 0), value=0)
        
        # âœ… å…³é”®ï¼šåº”ç”¨æ­£ç¡®çš„å½’ä¸€åŒ–
        # å°†[0,1]èŒƒå›´è½¬æ¢ä¸º[0,255]ï¼Œå†è¿›è¡Œå½’ä¸€åŒ–
        if input_image_torch.max() <= 1.0:
            input_image_torch = input_image_torch * 255.0
        
        # âœ… ä¿®å¤3ï¼šç¡®ä¿pixel_mean/stdåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        input_image_torch = (input_image_torch - self.pixel_mean) / self.pixel_std
        
        # âœ… å›¾åƒç¼–ç 
        image_embedding = self.model.image_encoder(input_image_torch)
        
        # ============================================================
        # âœ… é˜¶æ®µ2ï¼šPromptå¤„ç†
        # ============================================================
        # âœ… å…³é”®ï¼šbboxåæ ‡å˜æ¢å¿…é¡»ä½¿ç”¨å®˜æ–¹ transform.apply_boxes()
        box_tensor = None
        if input_boxes is not None:
            # è½¬æ¢ä¸ºnumpyï¼ˆå› ä¸ºapply_boxesæœŸæœ›numpyè¾“å…¥ï¼‰
            if isinstance(input_boxes, (list, tuple, np.ndarray)):
                box_np = np.array(input_boxes).reshape(1, 4)
            else:  # tensor
                box_np = input_boxes.detach().cpu().numpy().reshape(1, 4)
            # âœ… ä½¿ç”¨å®˜æ–¹å˜æ¢ï¼ˆç²¾ç¡®å¤ç°ï¼‰
            box_tf = self.transform.apply_boxes(box_np, original_size)
            box_tensor = torch.tensor(box_tf, device=self.device).unsqueeze(0).float()
        
        # âœ… Promptç¼–ç 
        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
            points=None, 
            boxes=box_tensor, 
            masks=None
        )
        
        # ============================================================
        # âœ… é˜¶æ®µ3ï¼šMaskè§£ç ä¸åå¤„ç†
        # ============================================================
        low_res_masks, iou_predictions = self.model.mask_decoder(
            image_embeddings=image_embedding,
            image_pe=self.model.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
        )
        
        # âœ… åå¤„ç†ï¼šå…ˆæ’å€¼logitsåˆ°åŸå›¾å°ºå¯¸ï¼Œå†åº”ç”¨sigmoid
        # ä¿ç•™logitsç”¨äºlossè®¡ç®—ï¼ˆBCE_with_logitsæ›´ç¨³å®šï¼Œæ”¯æŒautocastï¼‰
        low_res_masks_resized = F.interpolate(
            low_res_masks, 
            size=original_size, 
            mode="bilinear", 
            align_corners=False
        )
        
        # åº”ç”¨sigmoidå¾—åˆ°æœ€ç»ˆé¢„æµ‹
        pred_masks = torch.sigmoid(low_res_masks_resized)
        
        # ============================================================
        # âœ… é˜¶æ®µ4ï¼šå¯¹æŠ—æ”»å‡»æŸå¤±è®¡ç®—ï¼ˆä½¿ç”¨logitsï¼Œæ”¯æŒmixed precisionï¼‰
        # ============================================================
        if target_mask is not None:
            if target_mask.shape != pred_masks.shape:
                target_mask = F.interpolate(
                    target_mask, 
                    size=pred_masks.shape[-2:], 
                    mode="bilinear", 
                    align_corners=False
                )
            # âœ… ä½¿ç”¨binary_cross_entropy_with_logitsï¼ˆautocastå®‰å…¨ï¼‰
            loss = F.binary_cross_entropy_with_logits(low_res_masks_resized, target_mask)
            return pred_masks, loss
        
        return pred_masks

    def predict(self, img_tensor, input_boxes=None):
        """æ¨ç†æ¨¡å¼ï¼ˆä¿æŒæ¥å£ä¸€è‡´ï¼‰"""
        with torch.no_grad():
            return self.forward(img_tensor, input_boxes)

# ---------------------------- æ¨¡å‹å·¥å‚ ----------------------------
MODEL_REGISTRY = {
    "medsam": MedSAMModel,
    "sam": SAMModel,
  	"sammed2d": SAMMed2DModel,
    "sam_point": SAMPointModel,
    "unet": UNetModel,
    "deeplab": DeepLabV3Model,
}

def create_model(model_name: str, device: str, model_cfg: dict) -> SegmentationModelBase:
    """æ¨¡å‹å·¥å‚å‡½æ•°"""
    if model_name not in MODEL_REGISTRY:
        raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}ï¼Œæ”¯æŒçš„æ¨¡å‹: {list(MODEL_REGISTRY.keys())}")
    
    model_class = MODEL_REGISTRY[model_name]
    return model_class(device, model_cfg)

def list_available_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    return list(MODEL_REGISTRY.keys())
